{"meta":{"title":"MapleStory","subtitle":null,"description":null,"author":"MapleStory.zeng","url":"http://yoursite.com"},"pages":[],"posts":[{"title":"Hello World","slug":"hello-world","date":"2021-02-17T11:16:02.940Z","updated":"2017-01-08T07:08:19.000Z","comments":true,"path":"2021/02/17/hello-world/","link":"","permalink":"http://yoursite.com/2021/02/17/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"New API for App Get Exit Reasons","slug":"New-API-for-App-Get-Exit-Reasons","date":"2020-08-30T14:47:12.000Z","updated":"2020-08-30T14:51:34.000Z","comments":true,"path":"2020/08/30/New-API-for-App-Get-Exit-Reasons/","link":"","permalink":"http://yoursite.com/2020/08/30/New-API-for-App-Get-Exit-Reasons/","excerpt":"","text":"ExampleCode:‘’’var mgr : ActivityManager = getSystemService(Context.ACTIVITY_SERVICE) as ActivityManagervar list : MutableList = mgr.getHistoricalProcessExitReasons(null, 0,0)Log.i(“Maple”, “ExitInfoList size:” + list.size);for(item in list) { Log.i(“Maple”, “Item:” + item.toString()); if (item.traceInputStream != null) { var reader = BufferedReader(item.traceInputStream?.reader()) var content:String try { content = reader.readText() Log.i(“Maple”, “Trace:” + content); }finally { reader.close() } }}‘’’ Results:‘’’2020-08-30 22:41:26.268 19557-19557/com.maplestory.moewallpaper I/Maple: list size:112020-08-30 22:41:26.277 19557-19557/com.maplestory.moewallpaper I/Maple: item:ApplicationExitInfo(timestamp=2020/8/30 下午10:41 pid=19390 realUid=10011 packageUid=10011 definingUid=10011 user=0 process=com.maplestory.moewallpaper reason=6 (ANR) status=0 importance=100 pss=80MB rss=143MB description=user request after error state=empty trace=null2020-08-30 22:41:26.281 19557-19557/com.maplestory.moewallpaper I/Maple: Trace: —– pid 19390 at 2020-08-30 22:41:14 —– Cmd line: com.maplestory.moewallpaper Build fingerprint: ‘google/walleye/walleye:11/RPB2.200611.009/6625208:user/release-keys’ ABI: ‘arm64’ Build type: optimized Zygote loaded classes=15764 post zygote classes=488 Dumping registered class loaders #0 dalvik.system.PathClassLoader: [], parent #1 #1 java.lang.BootClassLoader: [], no parent #2 dalvik.system.PathClassLoader: [/data/app/~~AJ5EyenErJb8hnAAxTgXcQ==/com.maplestory.moewallpaper-Q-dp2ZicMU7g5NGQgWQqoA==/base.apk:/data/app/~~AJ5EyenErJb8hnAAxTgXcQ==/com.maplestory.moewallpaper-Q-dp2ZicMU7g5NGQgWQqoA==/base.apk!classes2.dex], parent #1 Done dumping class loaders Classes initialized: 358 in 5.355ms Intern table: 31575 strong; 520 weak JNI: CheckJNI is on; globals=659 (plus 42 weak) Libraries: libandroid.so libaudioeffect_jni.so libcompiler_rt.so libicu_jni.so libjavacore.so libjavacrypto.so libjnigraphics.so libmedia_jni.so libopenjdk.so librs_jni.so libsfplugin_ccodec.so libsoundpool.so libstats_jni.so libwebviewchromium_loader.so (14) Heap: 44% free, 2645KB/4805KB; 68790 objects Dumping cumulative Gc timings Start Dumping histograms for 1 iterations for concurrent copying ProcessMarkStack: Sum: 3.267ms 99% C.I. 3.267ms-3.267ms Avg: 3.267ms Max: 3.267ms VisitConcurrentRoots: Sum: 3.017ms 99% C.I. 1.499ms-1.518ms Avg: 1.508ms Max: 1.518ms ScanImmuneSpaces: Sum: 2.299ms 99% C.I. 0.023ms-2.276ms Avg: 1.149ms Max: 2.276ms MarkingPhase: Sum: 1.505ms 99% C.I. 1.505ms-1.505ms Avg: 1.505ms Max: 1.505ms SweepSystemWeaks: Sum: 761us 99% C.I. 761us-761us Avg: 761us Max: 761us InitializePhase: Sum: 733us 99% C.I. 733us-733us Avg: 733us Max: 733us ClearFromSpace: Sum: 284us 99% C.I. 284us-284us Avg: 284us Max: 284us GrayAllDirtyImmuneObjects: Sum: 248us 99% C.I. 248us-248us Avg: 248us Max: 248us CaptureThreadRootsForMarking: Sum: 131us 99% C.I. 131us-131us Avg: 131us Max: 131us ScanCardsForSpace: Sum: 108us 99% C.I. 108us-108us Avg: 108us Max: 108us VisitNonThreadRoots: Sum: 93us 99% C.I. 43us-50us Avg: 46.500us Max: 50us EnqueueFinalizerReferences: Sum: 89us 99% C.I. 89us-89us Avg: 89us Max: 89us FlipOtherThreads: Sum: 76us 99% C.I. 76us-76us Avg: 76us Max: 76us MarkZygoteLargeObjects: Sum: 54us 99% C.I. 54us-54us Avg: 54us Max: 54us ProcessReferences: Sum: 49us 99% C.I. 1us-48us Avg: 24.500us Max: 48us SweepAllocSpace: Sum: 31us 99% C.I. 31us-31us Avg: 31us Max: 31us SweepLargeObjects: Sum: 29us 99% C.I. 29us-29us Avg: 29us Max: 29us MarkStackAsLive: Sum: 24us 99% C.I. 24us-24us Avg: 24us Max: 24us CopyingPhase: Sum: 16us 99% C.I. 16us-16us Avg: 16us Max: 16us (Paused)GrayAllNewlyDirtyImmuneObjects: Sum: 14us 99% C.I. 14us-14us Avg: 14us Max: 14us EmptyRBMarkBitStack: Sum: 12us 99% C.I. 12us-12us Avg: 12us Max: 12us ThreadListFlip: Sum: 11us 99% C.I. 11us-11us Avg: 11us Max: 11us ReclaimPhase: Sum: 9us 99% C.I. 9us-9us Avg: 9us Max: 9us (Paused)ClearCards: Sum: 3us 99% C.I. 250ns-2000ns Avg: 230ns Max: 2000ns (Paused)FlipCallback: Sum: 2us 99% C.I. 2us-2us Avg: 2us Max: 2us ResumeOtherThreads: Sum: 0 99% C.I. 0ns-0ns Avg: 0ns Max: 0ns Done Dumping histograms concurrent copying paused: Sum: 33us 99% C.I. 33us-33us Avg: 33us Max: 33us concurrent copying freed-bytes: Avg: 2803KB Max: 2803KB Min: 2803KB Freed-bytes histogram: 2560:1 concurrent copying total time: 13.028ms mean time: 13.028ms concurrent copying freed: 43322 objects with total size 2803KB concurrent copying throughput: 3.33246e+06/s / 210MB/s per cpu-time: 239198666/s / 228MB/s Average major GC reclaim bytes ratio 1.30548 over 1 GC cycles Average major GC copied live bytes ratio 0.622111 over 5 major GCs Cumulative bytes moved 12685104 Cumulative objects moved 227401 Peak regions allocated 27 (6912KB) / 768 (192MB) Average minor GC reclaim bytes ratio inf over 0 GC cycles Average minor GC copied live bytes ratio 0.135833 over 1 minor GCs Cumulative bytes moved 551920 Cumulative objects moved 10954 Peak regions allocated 27 (6912KB) / 768 (192MB) Total time spent in GC: 13.028ms 2020-08-30 22:41:26.281 19557-19557/com.maplestory.moewallpaper I/Maple: Mean GC size throughput: 210MB/s per cpu-time: 227MB/s Mean GC object throughput: 3.3253e+06 objects/s Total number of allocations 112112 Total bytes allocated 5448KB Total bytes freed 2803KB Free memory 2159KB Free memory until GC 2159KB Free memory until OOME 189MB Total memory 4805KB Max memory 192MB Zygote space size 3180KB Total mutator paused time: 33us Total time waiting for GC to complete: 5.779us Total GC count: 1 Total GC time: 13.028ms Total blocking GC count: 0 Total blocking GC time: 0 Native bytes total: 11104220 registered: 197580 Total native bytes at last GC: 10375464 /system/framework/oat/arm64/android.hidl.base-V1.0-java.odex: quicken /system/framework/oat/arm64/android.hidl.manager-V1.0-java.odex: quicken /system/framework/oat/arm64/android.test.base.odex: quicken Current JIT code cache size (used / resident): 28KB / 32KB Current JIT data cache size (used / resident): 50KB / 56KB Zygote JIT code cache size (at point of fork): 43KB / 48KB Zygote JIT data cache size (at point of fork): 40KB / 44KB Current JIT mini-debug-info size: 61KB Current JIT capacity: 64KB Current number of JIT JNI stub entries: 0 Current number of JIT code cache entries: 95 Total number of JIT compilations: 59 Total number of JIT compilations for on stack replacement: 0 Total number of JIT code cache collections: 0 Memory used for stack maps: Avg: 110B Max: 1368B Min: 16B Memory used for compiled code: Avg: 481B Max: 4928B Min: 20B Memory used for profiling info: Avg: 114B Max: 1736B Min: 32B Start Dumping histograms for 95 iterations for JIT timings Compiling: Sum: 87.678ms 99% C.I. 125us-5855us Avg: 922.926us Max: 6896us TrimMaps: Sum: 3.034ms 99% C.I. 8us-152.500us Avg: 31.936us Max: 189us Done Dumping histograms Memory used for compilation: Avg: 92KB Max: 604KB Min: 15KB ProfileSaver total_bytes_written=0 ProfileSaver total_number_of_writes=0 ProfileSaver total_number_of_code_cache_queries=0 ProfileSaver total_number_of_skipped_writes=0 ProfileSaver total_number_of_failed_writes=0 ProfileSaver total_ms_of_sleep=5000 ProfileSaver total_ms_of_work=0 ProfileSaver total_number_of_hot_spikes=0 ProfileSaver total_number_of_wake_ups=0 suspend all histogram: Sum: 80us 99% C.I. 1us-22us Avg: 7.272us Max: 22us DALVIK THREADS (14): &quot;Signal Catcher&quot; daemon prio=10 tid=6 Runnable | group=&quot;system&quot; sCount=0 dsCount=0 flags=0 obj=0x13140268 self=0x781b687be0 | sysTid=19402 nice=-20 cgrp=default sched=0/0 handle=0x76990c9cc0 | state=R schedstat=( 34897969 2962968 13 ) utm=1 stm=1 core=5 HZ=100 | stack=0x7698fd2000-0x7698fd4000 stackSize=995KB | held mutexes= &quot;mutator lock&quot;(shared held) native: #00 pc 00000000004a5474 /apex/com.android.art/lib64/libart.so (art::DumpNativeStack(std::__1::basic_ostream&lt;char, std::__1::char_traits&lt;char&gt; &gt;&amp;, int, BacktraceMap*, char const*, art::ArtMethod*, void*, bool)+140) native: #01 pc 00000000005b4334 /apex/com.android.art/lib64/libart.so (art::Thread::DumpStack(std::__1::basic_ostream&lt;char, std::__1::char_traits&lt;char&gt; &gt;&amp;, bool, BacktraceMap*, bool) const+372) native: #02 pc 00000000005d18d4 /apex/com.android.art/lib64/libart.so (art::DumpCheckpoint::Run(art::Thread*)+924) native: #03 pc 00000000005cb744 /apex/com.android.art/lib64/libart.so (art::ThreadList::RunCheckpoint(art::Closure*, art::Closure*)+532) native: #04 pc 00000000005ca8c4 /apex/com.android.art/lib64/libart.so (art::ThreadList::Dump(std::__1::basic_ostream&lt;char, std::__1::char_traits&lt;char&gt; &gt;&amp;, bool)+1876) native: #05 pc 00000000005c9d80 /apex/com.android.art/lib64/libart.so (art::ThreadList::DumpForSigQuit(std::__1::basic_ostream&lt;char, std::__1::char_traits&lt;char&gt; &gt;&amp;)+792) native: #06 pc 0000000000575124 /apex/com.android.art/lib64/libart.so (art::Runtime::DumpForSigQuit(std::__1::basic_ostream&lt;char, std::__1::char_traits&lt;char&gt; &gt;&amp;)+196) native: #07 pc 000000000058abac /apex/com.android.art/lib64/libart.so (art::SignalCatcher::HandleSigQuit()+1396) 2020-08-30 22:41:26.281 19557-19557/com.maplestory.moewallpaper I/Maple: native: #08 pc 0000000000589b44 /apex/com.android.art/lib64/libart.so (art::SignalCatcher::Run(void)+348) native: #09 pc 00000000000b05d8 /apex/com.android.runtime/lib64/bionic/libc.so (__pthread_start(void)+64) native: #10 pc 00000000000500d0 /apex/com.android.runtime/lib64/bionic/libc.so (__start_thread+64) (no managed stack frames) &quot;main&quot; prio=5 tid=1 Sleeping | group=&quot;main&quot; sCount=1 dsCount=0 flags=1 obj=0x722a11d8 self=0x781b686010 | sysTid=19390 nice=-10 cgrp=default sched=0/0 handle=0x7941eee4f8 | state=S schedstat=( 443395835 25955369 347 ) utm=38 stm=5 core=4 HZ=100 | stack=0x7ffac00000-0x7ffac02000 stackSize=8192KB | held mutexes= at java.lang.Thread.sleep(Native method) - sleeping on &lt;0x0c3332c8&gt; (a java.lang.Object) at java.lang.Thread.sleep(Thread.java:442) - locked &lt;0x0c3332c8&gt; (a java.lang.Object) at java.lang.Thread.sleep(Thread.java:358) at com.maplestory.moewallpaper.ui.dashboard.DashboardFragment$onCreateView$2.onClick(DashboardFragment.kt:33) at android.view.View.performClick(View.java:7448) at android.view.View.performClickInternal(View.java:7425) at android.view.View.access$3600(View.java:810) at android.view.View$PerformClick.run(View.java:28296) at android.os.Handler.handleCallback(Handler.java:938) at android.os.Handler.dispatchMessage(Handler.java:99) at android.os.Looper.loop(Looper.java:223) at android.app.ActivityThread.main(ActivityThread.java:7656) at java.lang.reflect.Method.invoke(Native method) at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:592) at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:947) &quot;perfetto_hprof_listener&quot; prio=10 tid=7 Native (still starting up) | group=&quot;&quot; sCount=1 dsCount=0 flags=1 obj=0x0 self=0x781b6a0140 | sysTid=19403 nice=-20 cgrp=default sched=0/0 handle=0x7697fcbcc0 | state=S schedstat=( 203540 0 8 ) utm=0 stm=0 core=5 HZ=100 | stack=0x7697ed4000-0x7697ed6000 stackSize=995KB | held mutexes= native: #00 pc 000000000009b224 /apex/com.android.runtime/lib64/bionic/libc.so (read+4) native: #01 pc 0000000000017e20 /apex/com.android.art/lib64/libperfetto_hprof.so (void* std::__1::__thread_proxy&lt;std::__1::tuple&lt;std::__1::unique_ptr&lt;std::__1::__thread_struct, std::__1::default_delete&lt;std::__1::__thread_struct&gt; &gt;, ArtPlugin_Initialize::$_29&gt; &gt;(void*)+280) native: #02 pc 00000000000b05d8 /apex/com.android.runtime/lib64/bionic/libc.so (__pthread_start(void*)+64) native: #03 pc 00000000000500d0 /apex/com.android.runtime/lib64/bionic/libc.so (__start_thread+64) (no managed stack frames) &quot;ADB-JDWP Connection Control Thread&quot; daemon prio=0 tid=8 WaitingInMainDebuggerLoop | group=&quot;system&quot; sCount=1 dsCount=0 flags=1 obj=0x131402e0 self=0x781b68eb20 | sysTid=19404 nice=-20 cgrp=default sched=0/0 handle=0x7697ecdcc0 | state=S schedstat=( 1635367 46823 14 ) utm=0 stm=0 core=5 HZ=100 | stack=0x7697dd6000-0x7697dd8000 stackSize=995KB | held mutexes= native: #00 pc 000000000009c568 /apex/com.android.runtime/lib64/bionic/libc.so (__ppoll+8) native: #01 pc 000000000005a27c /apex/com.android.runtime/lib64/bionic/libc.so (poll+92) native: #02 pc 0000000000009fc4 /apex/com.android.art/lib64/libadbconnection.so (adbconnection::AdbConnectionState::RunPollLoop(art::Thread*)+812) native: #03 pc 00000000000085f8 /apex/com.android.art/lib64/libadbconnection.so (adbconnection::CallbackFunction(void*)+1512) native: #04 pc 00000000000b05d8 /apex/com.android.runtime/lib64/bionic/libc.so (__pthread_start(void*)+64) native: #05 pc 00000000000500d0 /apex/com.android.runtime/lib64/bionic/libc.so (__start_thread+64) (no managed stack frames) &quot;Jit thread pool worker thread 0&quot; daemon prio=5 tid=9 Native | group=&quot;system&quot; sCount=1 dsCount=0 flags=1 obj=0x13140358 self=0x781b697630 | sysTid=19405 nice=0 cgrp=default sched=0/0 handle=0x764c29cd00 | state=S schedstat=( 47737405 6765880 85 ) utm=3 stm=0 core=5 HZ=100 | stack=0x764c19e000-0x764c1a0000 stackSize=1023KB 2020-08-30 22:41:26.281 19557-19557/com.maplestory.moewallpaper I/Maple: | held mutexes= native: #00 pc 000000000004b00c /apex/com.android.runtime/lib64/bionic/libc.so (syscall+28) native: #01 pc 00000000001b079c /apex/com.android.art/lib64/libart.so (art::ConditionVariable::WaitHoldingLocks(art::Thread)+148) native: #02 pc 00000000005d3630 /apex/com.android.art/lib64/libart.so (art::ThreadPool::GetTask(art::Thread)+120) native: #03 pc 00000000005d28b8 /apex/com.android.art/lib64/libart.so (art::ThreadPoolWorker::Run()+144) native: #04 pc 00000000005d2378 /apex/com.android.art/lib64/libart.so (art::ThreadPoolWorker::Callback(void)+192) native: #05 pc 00000000000b05d8 /apex/com.android.runtime/lib64/bionic/libc.so (__pthread_start(void)+64) native: #06 pc 00000000000500d0 /apex/com.android.runtime/lib64/bionic/libc.so (__start_thread+64) (no managed stack frames) &quot;HeapTaskDaemon&quot; daemon prio=5 tid=10 WaitingForTaskProcessor | group=&quot;system&quot; sCount=1 dsCount=0 flags=1 obj=0x13140790 self=0x781b69e570 | sysTid=19406 nice=4 cgrp=default sched=0/0 handle=0x764b197cc0 | state=S schedstat=( 16350470 1177184 40 ) utm=0 stm=0 core=4 HZ=100 | stack=0x764b094000-0x764b096000 stackSize=1043KB | held mutexes= native: #00 pc 000000000004b00c /apex/com.android.runtime/lib64/bionic/libc.so (syscall+28) native: #01 pc 00000000001b079c /apex/com.android.art/lib64/libart.so (art::ConditionVariable::WaitHoldingLocks(art::Thread*)+148) native: #02 pc 00000000002e86a4 /apex/com.android.art/lib64/libart.so (art::gc::TaskProcessor::GetTask(art::Thread*)+548) native: #03 pc 00000000002e8ff4 /apex/com.android.art/lib64/libart.so (art::gc::TaskProcessor::RunAllTasks(art::Thread*)+92) at dalvik.system.VMRuntime.runHeapTasks(Native method) at java.lang.Daemons$HeapTaskDaemon.runInternal(Daemons.java:531) at java.lang.Daemons$Daemon.run(Daemons.java:139) at java.lang.Thread.run(Thread.java:923) &quot;ReferenceQueueDaemon&quot; daemon prio=5 tid=11 Waiting | group=&quot;system&quot; sCount=1 dsCount=0 flags=1 obj=0x131403d0 self=0x781b69add0 | sysTid=19407 nice=4 cgrp=default sched=0/0 handle=0x764a08dcc0 | state=S schedstat=( 397761 34999 5 ) utm=0 stm=0 core=4 HZ=100 | stack=0x7649f8a000-0x7649f8c000 stackSize=1043KB | held mutexes= at java.lang.Object.wait(Native method) - waiting on &lt;0x09f5bf61&gt; (a java.lang.Class&lt;java.lang.ref.ReferenceQueue&gt;) at java.lang.Object.wait(Object.java:442) at java.lang.Object.wait(Object.java:568) at java.lang.Daemons$ReferenceQueueDaemon.runInternal(Daemons.java:217) - locked &lt;0x09f5bf61&gt; (a java.lang.Class&lt;java.lang.ref.ReferenceQueue&gt;) at java.lang.Daemons$Daemon.run(Daemons.java:139) at java.lang.Thread.run(Thread.java:923) &quot;FinalizerDaemon&quot; daemon prio=5 tid=12 Waiting | group=&quot;system&quot; sCount=1 dsCount=0 flags=1 obj=0x13140448 self=0x781b69c9a0 | sysTid=19408 nice=4 cgrp=default sched=0/0 handle=0x7649f83cc0 | state=S schedstat=( 702865 110207 8 ) utm=0 stm=0 core=5 HZ=100 | stack=0x7649e80000-0x7649e82000 stackSize=1043KB | held mutexes= at java.lang.Object.wait(Native method) - waiting on &lt;0x03f83a86&gt; (a java.lang.Object) at java.lang.Object.wait(Object.java:442) at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:190) - locked &lt;0x03f83a86&gt; (a java.lang.Object) at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:211) at java.lang.Daemons$FinalizerDaemon.runInternal(Daemons.java:273) at java.lang.Daemons$Daemon.run(Daemons.java:139) at java.lang.Thread.run(Thread.java:923) &quot;FinalizerWatchdogDaemon&quot; daemon prio=5 tid=13 Sleeping | group=&quot;system&quot; sCount=1 dsCount=0 flags=1 obj=0x131404c0 self=0x781b6a1d10 | sysTid=19409 nice=4 cgrp=default sched=0/0 handle=0x7647e79cc0 | state=S schedstat=( 139950 234219 11 ) utm=0 stm=0 core=4 HZ=100 | stack=0x7647d76000-0x7647d78000 stackSize=1043KB | held mutexes= at java.lang.Thread.sleep(Native method) - sleeping on &lt;0x0c29ef47&gt; (a java.lang.Object) at java.lang.Thread.sleep(Thread.java:442) - locked &lt;0x0c29ef47&gt; (a java.lang.Object) 2020-08-30 22:41:26.281 19557-19557/com.maplestory.moewallpaper I/Maple: at java.lang.Thread.sleep(Thread.java:358) at java.lang.Daemons$FinalizerWatchdogDaemon.sleepForNanos(Daemons.java:390) at java.lang.Daemons$FinalizerWatchdogDaemon.waitForFinalization(Daemons.java:419) at java.lang.Daemons$FinalizerWatchdogDaemon.runInternal(Daemons.java:325) at java.lang.Daemons$Daemon.run(Daemons.java:139) at java.lang.Thread.run(Thread.java:923) &quot;Binder:19390_1&quot; prio=5 tid=14 Native | group=&quot;main&quot; sCount=1 dsCount=0 flags=1 obj=0x13140538 self=0x781b6a7080 | sysTid=19413 nice=0 cgrp=default sched=0/0 handle=0x7646c71cc0 | state=S schedstat=( 12456981 6410729 53 ) utm=0 stm=0 core=0 HZ=100 | stack=0x7646b7a000-0x7646b7c000 stackSize=995KB | held mutexes= native: #00 pc 000000000009b4a4 /apex/com.android.runtime/lib64/bionic/libc.so (__ioctl+4) native: #01 pc 0000000000057bb8 /apex/com.android.runtime/lib64/bionic/libc.so (ioctl+160) native: #02 pc 0000000000050c6c /system/lib64/libbinder.so (android::IPCThreadState::talkWithDriver(bool)+300) native: #03 pc 0000000000050e60 /system/lib64/libbinder.so (android::IPCThreadState::getAndExecuteCommand()+24) native: #04 pc 0000000000051724 /system/lib64/libbinder.so (android::IPCThreadState::joinThreadPool(bool)+60) native: #05 pc 0000000000077b60 /system/lib64/libbinder.so (android::PoolThread::threadLoop()+24) native: #06 pc 000000000001564c /system/lib64/libutils.so (android::Thread::_threadLoop(void*)+260) native: #07 pc 00000000000a0d3c /system/lib64/libandroid_runtime.so (android::AndroidRuntime::javaThreadShell(void*)+140) native: #08 pc 0000000000014ee4 /system/lib64/libutils.so (thread_data_t::trampoline(thread_data_t const*)+412) native: #09 pc 00000000000b05d8 /apex/com.android.runtime/lib64/bionic/libc.so (__pthread_start(void*)+64) native: #10 pc 00000000000500d0 /apex/com.android.runtime/lib64/bionic/libc.so (__start_thread+64) (no managed stack frames) &quot;Binder:19390_2&quot; prio=5 tid=15 Native | group=&quot;main&quot; sCount=1 dsCount=0 flags=1 obj=0x131405b0 self=0x781b6a54b0 | sysTid=19414 nice=0 cgrp=default sched=0/0 handle=0x7645b73cc0 | state=S schedstat=( 555782 327865 11 ) utm=0 stm=0 core=5 HZ=100 | stack=0x7645a7c000-0x7645a7e000 stackSize=995KB | held mutexes= native: #00 pc 000000000009b4a4 /apex/com.android.runtime/lib64/bionic/libc.so (__ioctl+4) native: #01 pc 0000000000057bb8 /apex/com.android.runtime/lib64/bionic/libc.so (ioctl+160) native: #02 pc 0000000000050c6c /system/lib64/libbinder.so (android::IPCThreadState::talkWithDriver(bool)+300) native: #03 pc 0000000000050e60 /system/lib64/libbinder.so (android::IPCThreadState::getAndExecuteCommand()+24) native: #04 pc 0000000000051724 /system/lib64/libbinder.so (android::IPCThreadState::joinThreadPool(bool)+60) native: #05 pc 0000000000077b60 /system/lib64/libbinder.so (android::PoolThread::threadLoop()+24) native: #06 pc 000000000001564c /system/lib64/libutils.so (android::Thread::_threadLoop(void*)+260) native: #07 pc 00000000000a0d3c /system/lib64/libandroid_runtime.so (android::AndroidRuntime::javaThreadShell(void*)+140) native: #08 pc 0000000000014ee4 /system/lib64/libutils.so (thread_data_t::trampoline(thread_data_t const*)+412) native: #09 pc 00000000000b05d8 /apex/com.android.runtime/lib64/bionic/libc.so (__pthread_start(void*)+64) native: #10 pc 00000000000500d0 /apex/com.android.runtime/lib64/bionic/libc.so (__start_thread+64) (no managed stack frames) &quot;Binder:19390_3&quot; prio=5 tid=16 Native | group=&quot;main&quot; sCount=1 dsCount=0 flags=1 obj=0x13140628 self=0x781b6a38e0 | sysTid=19415 nice=0 cgrp=default sched=0/0 handle=0x7644a75cc0 | state=S schedstat=( 822032 2271563 13 ) utm=0 stm=0 core=5 HZ=100 | stack=0x764497e000-0x7644980000 stackSize=995KB | held mutexes= native: #00 pc 000000000009b4a4 /apex/com.android.runtime/lib64/bionic/libc.so (__ioctl+4) native: #01 pc 0000000000057bb8 /apex/com.android.runtime/lib64/bionic/libc.so (ioctl+160) 2020-08-30 22:41:26.281 19557-19557/com.maplestory.moewallpaper I/Maple: native: #02 pc 0000000000050c6c /system/lib64/libbinder.so (android::IPCThreadState::talkWithDriver(bool)+300) native: #03 pc 0000000000050e60 /system/lib64/libbinder.so (android::IPCThreadState::getAndExecuteCommand()+24) native: #04 pc 0000000000051724 /system/lib64/libbinder.so (android::IPCThreadState::joinThreadPool(bool)+60) native: #05 pc 0000000000077b60 /system/lib64/libbinder.so (android::PoolThread::threadLoop()+24) native: #06 pc 000000000001564c /system/lib64/libutils.so (android::Thread::_threadLoop(void)+260) native: #07 pc 00000000000a0d3c /system/lib64/libandroid_runtime.so (android::AndroidRuntime::javaThreadShell(void)+140) native: #08 pc 0000000000014ee4 /system/lib64/libutils.so (thread_data_t::trampoline(thread_data_t const)+412) native: #09 pc 00000000000b05d8 /apex/com.android.runtime/lib64/bionic/libc.so (__pthread_start(void)+64) native: #10 pc 00000000000500d0 /apex/com.android.runtime/lib64/bionic/libc.so (__start_thread+64) (no managed stack frames) &quot;Profile Saver&quot; daemon prio=5 tid=17 Native | group=&quot;system&quot; sCount=1 dsCount=0 flags=1 obj=0x131406a0 self=0x781b6adfc0 | sysTid=19435 nice=9 cgrp=default sched=0/0 handle=0x764339fcc0 | state=S schedstat=( 19171664 2131980 10 ) utm=1 stm=0 core=4 HZ=100 | stack=0x76432a8000-0x76432aa000 stackSize=995KB | held mutexes= native: #00 pc 000000000004b00c /apex/com.android.runtime/lib64/bionic/libc.so (syscall+28) native: #01 pc 00000000001b079c /apex/com.android.art/lib64/libart.so (art::ConditionVariable::WaitHoldingLocks(art::Thread*)+148) native: #02 pc 000000000035805c /apex/com.android.art/lib64/libart.so (art::ProfileSaver::Run()+484) native: #03 pc 000000000035ce48 /apex/com.android.art/lib64/libart.so (art::ProfileSaver::RunProfileSaverThread(void*)+176) native: #04 pc 00000000000b05d8 /apex/com.android.runtime/lib64/bionic/libc.so (__pthread_start(void*)+64) native: #05 pc 00000000000500d0 /apex/com.android.runtime/lib64/bionic/libc.so (__start_thread+64) (no managed stack frames) &quot;RenderThread&quot; daemon prio=7 tid=18 Native | group=&quot;main&quot; sCount=1 dsCount=0 flags=1 obj=0x13140718 self=0x781b6ac3f0 | sysTid=19436 nice=-10 cgrp=default sched=0/0 handle=0x76422a1cc0 | state=S schedstat=( 193720468 12791772 280 ) utm=14 stm=4 core=4 HZ=100 | stack=0x76421aa000-0x76421ac000 stackSize=995KB | held mutexes= native: #00 pc 000000000009c468 /apex/com.android.runtime/lib64/bionic/libc.so (__epoll_pwait+8) native: #01 pc 0000000000019d58 /system/lib64/libutils.so (android::Looper::pollInner(int)+184) native: #02 pc 0000000000019c38 /system/lib64/libutils.so (android::Looper::pollOnce(int, int*, int*, void**)+112) native: #03 pc 000000000020fe34 /system/lib64/libhwui.so (android::uirenderer::ThreadBase::waitForWork()+132) native: #04 pc 0000000000231678 /system/lib64/libhwui.so (android::uirenderer::renderthread::RenderThread::threadLoop()+80) native: #05 pc 000000000001564c /system/lib64/libutils.so (android::Thread::_threadLoop(void*)+260) native: #06 pc 0000000000014ee4 /system/lib64/libutils.so (thread_data_t::trampoline(thread_data_t const*)+412) native: #07 pc 00000000000b05d8 /apex/com.android.runtime/lib64/bionic/libc.so (__pthread_start(void*)+64) native: #08 pc 00000000000500d0 /apex/com.android.runtime/lib64/bionic/libc.so (__start_thread+64) (no managed stack frames) ----- end 19390 ----- ----- Waiting Channels: pid 19390 at 2020-08-30 22:41:14 ----- Cmd line: com.maplestory.moewallpaper sysTid=19390 futex_wait_queue_me sysTid=19402 do_sigtimedwait sysTid=19403 pipe_read sysTid=19404 do_sys_poll sysTid=19405 futex_wait_queue_me sysTid=19406 futex_wait_queue_me sysTid=19407 futex_wait_queue_me sysTid=19408 futex_wait_queue_me sysTid=19409 futex_wait_queue_me sysTid=19413 binder_thread_read sysTid=19414 binder_thread_read sysTid=19415 binder_thread_read sysTid=19435 futex_wait_queue_me sysTid=19436 SyS_epoll_wait 2020-08-30 22:41:26.281 19557-19557/com.maplestory.moewallpaper I/Maple: —– end 19390 —–2020-08-30 22:41:26.282 19557-19557/com.maplestory.moewallpaper I/Maple: item:ApplicationExitInfo(timestamp=2020/8/30 下午10:41 pid=19224 realUid=10011 packageUid=10011 definingUid=10011 user=0 process=com.maplestory.moewallpaper reason=10 (USER REQUESTED) status=0 importance=100 pss=76MB rss=140MB description=stop com.maplestory.moewallpaper due to from pid 19333 state=empty trace=null2020-08-30 22:41:26.284 19557-19557/com.maplestory.moewallpaper I/Maple: item:ApplicationExitInfo(timestamp=2020/8/30 下午10:40 pid=19081 realUid=10011 packageUid=10011 definingUid=10011 user=0 process=com.maplestory.moewallpaper reason=10 (USER REQUESTED) status=0 importance=100 pss=57MB rss=118MB description=stop com.maplestory.moewallpaper due to from pid 19169 state=empty trace=null2020-08-30 22:41:26.285 19557-19557/com.maplestory.moewallpaper I/Maple: item:ApplicationExitInfo(timestamp=2020/8/30 下午10:38 pid=18926 realUid=10011 packageUid=10011 definingUid=10011 user=0 process=com.maplestory.moewallpaper reason=4 (APP CRASH(EXCEPTION)) status=0 importance=100 pss=0.00 rss=0.00 description=crash state=empty trace=null2020-08-30 22:41:26.286 19557-19557/com.maplestory.moewallpaper I/Maple: item:ApplicationExitInfo(timestamp=2020/8/30 下午10:38 pid=18718 realUid=10011 packageUid=10011 definingUid=10011 user=0 process=com.maplestory.moewallpaper reason=10 (USER REQUESTED) status=0 importance=100 pss=54MB rss=112MB description=stop com.maplestory.moewallpaper due to from pid 18864 state=empty trace=null2020-08-30 22:41:26.287 19557-19557/com.maplestory.moewallpaper I/Maple: item:ApplicationExitInfo(timestamp=2020/8/30 下午10:34 pid=18584 realUid=10011 packageUid=10011 definingUid=10011 user=0 process=com.maplestory.moewallpaper reason=4 (APP CRASH(EXCEPTION)) status=0 importance=100 pss=57MB rss=119MB description=crash state=empty trace=null2020-08-30 22:41:26.288 19557-19557/com.maplestory.moewallpaper I/Maple: item:ApplicationExitInfo(timestamp=2020/8/30 下午10:33 pid=18428 realUid=10011 packageUid=10011 definingUid=10011 user=0 process=com.maplestory.moewallpaper reason=10 (USER REQUESTED) status=0 importance=100 pss=57MB rss=119MB description=stop com.maplestory.moewallpaper due to from pid 18523 state=empty trace=null2020-08-30 22:41:26.289 19557-19557/com.maplestory.moewallpaper I/Maple: item:ApplicationExitInfo(timestamp=2020/8/30 下午10:19 pid=17902 realUid=10011 packageUid=10011 definingUid=10011 user=0 process=com.maplestory.moewallpaper reason=4 (APP CRASH(EXCEPTION)) status=0 importance=100 pss=0.00 rss=0.00 description=crash state=empty trace=null2020-08-30 22:41:26.290 19557-19557/com.maplestory.moewallpaper I/Maple: item:ApplicationExitInfo(timestamp=2020/8/30 下午10:13 pid=17195 realUid=10011 packageUid=10011 definingUid=10011 user=0 process=com.maplestory.moewallpaper reason=4 (APP CRASH(EXCEPTION)) status=0 importance=100 pss=0.00 rss=0.00 description=crash state=empty trace=null2020-08-30 22:41:26.291 19557-19557/com.maplestory.moewallpaper I/Maple: item:ApplicationExitInfo(timestamp=2020/8/30 下午10:13 pid=16841 realUid=10011 packageUid=10011 definingUid=10011 user=0 process=com.maplestory.moewallpaper reason=10 (USER REQUESTED) status=0 importance=100 pss=73MB rss=137MB description=stop com.maplestory.moewallpaper due to from pid 17101 state=empty trace=null2020-08-30 22:41:26.292 19557-19557/com.maplestory.moewallpaper I/Maple: item:ApplicationExitInfo(timestamp=2020/8/30 下午10:07 pid=16577 realUid=10011 packageUid=10011 definingUid=10011 user=0 process=com.maplestory.moewallpaper reason=10 (USER REQUESTED) status=0 importance=100 pss=57MB rss=120MB description=stop com.maplestory.moewallpaper due to from pid 16766 state=empty trace=null ‘’’","categories":[],"tags":[]},{"title":"Touch Event Dispatch Procedure in Android 10","slug":"Input-Dispatch-Procedure-in-Android-10","date":"2019-10-25T09:32:19.000Z","updated":"2019-10-25T14:21:17.000Z","comments":true,"path":"2019/10/25/Input-Dispatch-Procedure-in-Android-10/","link":"","permalink":"http://yoursite.com/2019/10/25/Input-Dispatch-Procedure-in-Android-10/","excerpt":"","text":"触摸事件只会分发给可见的能够接受事件的窗口 在P版本上相关的成员ViewRootImplWindowManagerServiceInputDispatcher Q版本相关的成员ViewRootImplWindowManagerServiceSurfaceFlingerInputDispatcher 当前窗口在执行完onResume后由框架向WindowManagerService添加窗口，并开始接收Input事件 123456789101112ActivityThread.javawm.addView(decor, l);ViewRootImpl.javares = mWindowSession.addToDisplay(mWindow, mSeq, mWindowAttributes, getHostVisibility(), mDisplay.getDisplayId(), mTmpFrame, mAttachInfo.mContentInsets, mAttachInfo.mStableInsets, mAttachInfo.mOutsets, mAttachInfo.mDisplayCutout, mInputChannel, mTempInsets);mInputEventReceiver = new WindowInputEventReceiver(mInputChannel, Looper.myLooper()); WindowManagerService添加窗口打开InputChannel更新焦点窗口更新Input窗口 1234567891011121314151617181920final boolean openInputChannels = (outInputChannel != null &amp;&amp; (attrs.inputFeatures &amp; INPUT_FEATURE_NO_INPUT_CHANNEL) == 0);if (openInputChannels) &#123; win.openInputChannel(outInputChannel);&#125;boolean focusChanged = false;if (win.canReceiveKeys()) &#123; focusChanged = updateFocusedWindowLocked(UPDATE_FOCUS_WILL_ASSIGN_LAYERS, false /*updateInputWindows*/); if (focusChanged) &#123; imMayMove = false; &#125;&#125;if (focusChanged) &#123; displayContent.getInputMonitor().setInputFocusLw(displayContent.mCurrentFocus, false /*updateInputWindows*/);&#125;displayContent.getInputMonitor().updateInputWindowsLw(false /*force*/); 焦点窗口被设置到SurfaceFlinger 12345678910111213141516171819202122232425262728293031323334353637383940414243// DisplayContent.javaint focusChanged = getDisplayPolicy().focusChangedLw(oldFocus, newFocus);// InputMonitor.javavoid updateInputWindowsLw(boolean force) &#123; if (!force &amp;&amp; !mUpdateInputWindowsNeeded) &#123; return; &#125; scheduleUpdateInputWindows();&#125;mUpdateInputForAllWindowsConsumer.updateInputWindows(inDrag);// 不更新的场景if (inputChannel == null || inputWindowHandle == null || w.mRemoved || w.cantReceiveTouchInput()) &#123; if (w.mWinAnimator.hasSurface()) &#123; mInputTransaction.setInputWindowInfo( w.mWinAnimator.mSurfaceController.mSurfaceControl, mInvalidInputWindow); &#125; // Skip this window because it cannot possibly receive input. return;&#125;// 更新的场景populateInputWindowHandle( inputWindowHandle, w, flags, type, isVisible, hasFocus, hasWallpaper);if (w.mWinAnimator.hasSurface()) &#123; mInputTransaction.setInputWindowInfo( w.mWinAnimator.mSurfaceController.mSurfaceControl, inputWindowHandle);&#125;// android_view_SurfaceControl.cppstatic void nativeSetInputWindowInfo(JNIEnv* env, jclass clazz, jlong transactionObj, jlong nativeObject, jobject inputWindow) &#123; auto transaction = reinterpret_cast&lt;SurfaceComposerClient::Transaction*&gt;(transactionObj); sp&lt;NativeInputWindowHandle&gt; handle = android_view_InputWindowHandle_getHandle( env, inputWindow); handle-&gt;updateInfo(); SurfaceControl* const ctrl = reinterpret_cast&lt;SurfaceControl *&gt;(nativeObject); transaction-&gt;setInputWindowInfo(ctrl, *handle-&gt;getInfo());&#125; SurfaceFlinger设置可以接收事件的窗口到InputDispatcher 123456789101112131415void SurfaceFlinger::updateInputWindowInfo() &#123; std::vector&lt;InputWindowInfo&gt; inputHandles; mDrawingState.traverseInReverseZOrder([&amp;](Layer* layer) &#123; if (layer-&gt;hasInput()) &#123; // When calculating the screen bounds we ignore the transparent region since it may // result in an unwanted offset. inputHandles.push_back(layer-&gt;fillInputInfo()); &#125; &#125;); mInputFlinger-&gt;setInputWindows(inputHandles, mInputWindowCommands.syncInputWindows ? mSetInputWindowsListener : nullptr);&#125; InputDispatcher根据InputWIndowHandle的属性更新分发窗口","categories":[],"tags":[]},{"title":"监控与故障管理(5)","slug":"监控与故障管理-5","date":"2019-10-07T11:17:57.000Z","updated":"2019-10-07T11:46:30.000Z","comments":true,"path":"2019/10/07/监控与故障管理-5/","link":"","permalink":"http://yoursite.com/2019/10/07/监控与故障管理-5/","excerpt":"","text":"终端监控与管理与云服务中的监控类似，主要是对流程中的日志进行记录，在故障发生前或发生是发出告警，并将相关联的日志分析，给出大概的建议。当然，不同点也很多。云服务日志的记录、分析及告警主要消耗的是服务提供者的算力，而终端的日志记录上传消耗的是用户的算力及流量。参考前面对故障监控管理模式以及AOSP的分析，下面主要讨论一个通用终端监控系统所具备的功能及各模块的职责。","categories":[],"tags":[]},{"title":"监控与故障管理(4)","slug":"监控与故障管理-4","date":"2019-10-07T09:06:57.000Z","updated":"2019-10-07T11:11:04.000Z","comments":true,"path":"2019/10/07/监控与故障管理-4/","link":"","permalink":"http://yoursite.com/2019/10/07/监控与故障管理-4/","excerpt":"","text":"从上面几篇叙述中可以了解到通用故障检测工具箱里应有的工具。下面来简要描述App生命周期里与此相关的工具开发阶段：1.编程语言自带以及Os扩展的异常处理机制Jdk UncaughtExceptionAndroid AndroidException及子类2.运行期状态获取接口（Debug、Status、Usage）3.编译器支持的Sanitizer4.Android Studio及集成的工具 调优阶段：在开发者者选项中各个模块为调试所添加的开关，如1.布局层级2.渲染时间独立提供的工具：如adb，dumpsys，systrace，perfetto最佳实践检查工具：StrictMode API 测试阶段：如1.Monkey2.性能测试（benchmark Api） 运维阶段：由firebase提供的Android Vitals服务包括崩溃，卡顿，功耗，数据访问延时监控的服务这里值得一提的是Android Vitals提供的服务与框架并无耦合，与其他三方服务提供的监控方式类似。 在以上工具以及采集的数据大部分是由框架中的组件采集的。例如1.debuggerd，tombstoned以及crashdump所组成的进程栈抓取工具2.ANR组成的进程任务执行响应监控3.SystemServer中的Watchdog监控关键资源响应延时4.ART中提供的Debug功能当然最近几个版本新 加入的一些工具的作用还未曾知晓，如statsdAndroidX与Android Studio也是AOSP的一部分。","categories":[],"tags":[]},{"title":"监控与故障管理(3)","slug":"监控与故障管理-3","date":"2019-09-25T14:32:19.000Z","updated":"2019-10-07T09:07:01.000Z","comments":true,"path":"2019/09/25/监控与故障管理-3/","link":"","permalink":"http://yoursite.com/2019/09/25/监控与故障管理-3/","excerpt":"","text":"错误恢复模式（参考书籍Patterns For Fault Tolerant Software第6章）：1.故障隔离（Quarantine）对故障的服务或者设备进行隔离，例如代理其提供的服务2.集中恢复（Concentrated Recovery）对重要恢复有显著的用户提示，避免误操作导致无法恢复的错误3.Error Handler错误处理器，例如UncautghtExceptionHandler4.重启重启进程，进程组或系统5.回滚返回进程、系统执行路径的前一个状态，当然，错误可能还会再次发生6.放通忽略掉错误7.返回参考点例如进程崩溃，进行了重启8.有限次数重试例如网络访问超时，再次发起请求9.主备切换10.检查点周期保存进程或系统状态，对于单进程比较容易，但对于进程组或整机而言，需要选择一个时机进行状态保存。当然，对于有依赖的进程，可以选择通过一系列依赖调用来进行保存。并且，保存的内容要恰好满足状态恢复的要求。这就要求设计上把状态数据与其他数据区分开来。11.数据重置清除用户数据以上恢复的方式是不是很眼熟，例如模式2，10在微软的Windows上都有对应的服务。大部分编程语言都会提供Exception Handler的机制。对于Android而言，模式2、4、7、11也有对应的机制或服务。 容错模式（参考书籍Patterns For Fault Tolerant Software第7章）：恢复模式会改变当前系统的状态，而容错模式保持当前系统状态而缓解因为故障带来的影响。例如对于请求过载型可以采用以下模式1.队列保持当前请求2.负载均衡3.丢弃部分请求对于联网应用来说对于网络数据的校验也是一种容错模式。","categories":[],"tags":[]},{"title":"监控与故障管理(2)","slug":"监控与故障管理-2","date":"2019-09-23T14:03:34.000Z","updated":"2019-09-25T14:30:09.000Z","comments":true,"path":"2019/09/23/监控与故障管理-2/","link":"","permalink":"http://yoursite.com/2019/09/23/监控与故障管理-2/","excerpt":"","text":"监控运行实体：无论对于系统监控抑或应用监控，监控应该在部署在任意可能出现问题的位置。如driver、kernel、service、framework以及应用本身。监控可以是异常处理代码中的一段日志，也可以是很长逻辑结果的一个校验。可以是重要事件的记录，也可以是当前状态的记录。可以运行在当前执行线程中，也可以是线程外甚至进程外。如果能够在任意两行代码之间记录日志，我们便能够通过日志找到任意两行之间的问题。综合性能的考虑，监控的目标是在尽可能少的记录的基础上尽可能多的推导出问题出现的原因。 监控与故障模式：监控只能是对已知模式的监控，如何在不同层次上有效率的检查与处理故障是监控与故障管理需要思考的核心问题。对于未知的问题有两种策略1.尽可能的覆盖，运气好的话能够直接记录到故障的现场。2.在问题发生后能根据记录大致判断故障位置，再使用覆盖法。整体上看需要有机制能够支撑以上的策略，例如动态开关，单独打开单个设备日志的能力。从故障理论来看，监控最小的层级是Fault，例如编码错误，设计错误以及需求错误，这些错误通常是不能被直接监控的。一些编码错误类的Fault能够通过静态或者动态检查工具完成，如Address Sanitizer、Thread Sanitizer。存在与代码中的Fault被激活后变成Error或者Exception，我们一般是在这个层级做监控。如果Error或者Exception没有被正确的处理，则会最终变成失效。我们希望能够直接处理Fault，事实上我们多数情况只能监控Error或者Failure。举个例子，AOSP的ANR是一种对Failure的监控。Crash收对一种Error的监控。而Address Sanitizer是一种对Fault的监控。 常见模式的监控方法（参考书籍Patterns For Fault Tolerant Software第五章）：1.故障关联（Fault Correlation）通过一系列的故障关联组合推导出故障的原因2.故障隔离 （Error Containment Barrier）对检测的到的故障进行隔离，执行一系列的恢复操作如回滚，重启3.事件完成校验 （Complete Parameter Checking）在事件执行前后进行检查，校验其运行结果4.系统事件监控 (System Monitor)对流程中的序列进行校验，对于不符合预期的序列进行操作5.heartbeat、acknowledgement以及watchdog这里将这三个放在一起，是因为都是对时间内运行状态的预期。6.恰当的阈值 (Realistic Threshold)阈值是对过载的一种检查，这里用恰当是因为我们在设计时对系统的规划通常会随着发展而变化，如何或者恰当的阈值是值得探讨的问题。7.状态检查 (Existing Metrics)检查目前已经存在的指标是否满足要求或在正常范围内8.日常维护 (Rotine Maintenance)周期性执行检查以避免故障积累导致失效9.日常审计 (Routine Audits)审计日志或其他记录以保证历史执行正确10.泄漏记录 (Leaky Bucket Counter)一种自动对错误增加或减少的记录器，例如资源使用与释放 综上，我们可以看出，一个监控系统应该提供的通用工具有0.对单点错误的统计1.对事件完成度的检查2.对事件序列完成度的检查3.对事件执行时间的检查4.对事件执行频次的检查6.对系统中各模块的状态检查7.对系统中各模块的日常维护，记录及审计8.对以上事件的综合判断9.对判断结果进行一系列的操作，以降低不可用的时间。","categories":[],"tags":[]},{"title":"监控与故障管理（1）","slug":"监控与故障管理","date":"2019-09-17T14:16:03.000Z","updated":"2019-09-23T14:01:58.000Z","comments":true,"path":"2019/09/17/监控与故障管理/","link":"","permalink":"http://yoursite.com/2019/09/17/监控与故障管理/","excerpt":"","text":"一般有追求的软件公司在业务范围、或团队规模扩大后都会引入所谓的监控系统，用于监控已有系统的运行状态，以支撑各种问题的预测与定位。在web服务领域中，APM（应用性能分析）的使用或者DevOps方式运作已经相当普遍了，例如阿里有鹰眼系统用于支撑其支付淘宝业务，腾讯有bugly支撑微信及各种手游。 谷歌总结其DevOps的实践经验整理成SRE运维解密（2012）。同年，gartner将APM作为一个技术热点详细分析了其商业模型。如今（2019），APM已经是一个近50亿美金的市场。 那么APM或者DevOps与监控、故障管理又是什么关系呢。举个例子。当一个系统只靠一个人就可以开发维护时，确实不需要一个系统专门来采集数据，管理故障，因为靠问题描述与走读代码即可很快找到原因。随着业务的扩展，人员的增加，可能需要多个组件配合才能完成一个功能时，问题就随之而来。当最终用户在某个服务提供的页面操作后没有及时响应，我们如何来定位与修复这个问题。是这个页面js没有响应，还是用户与服务器的连接质量不佳丢包严重，或者是服务器用户量太大，导致过载。如果问题频繁出现势必会导致用户体验的下降，从而损失（用户），而APM就是为了解决复杂系统内潜在的模块问题，模块间问题而产生的。APM作为一种服务，监控一个系统运行的全流程，当发生故障时能够预警，并提供支撑定位的日志信息。 那么一个APM系统应当具有哪些功能与特性呢？1.监控与收集Metrics 监控点容易添加，系统开销小2.统计数据展示 清楚明了的展示系统变化趋势，不同角色有不同的展示逻辑 展示逻辑清晰3.对于异常能够及时预警 能够自动对问题按既定规则进行分级，展示问题发生的过程 预警能自定义 报告能附带上下文，重新组织收集的信息而不是一整包的提供，更不要分散的提供4.能够与其他系统联动，闭环问题 与问题跟踪系统联动 与客户服务系统联动 与数据挖掘系统联动 与恢复系统联动 虽说都是复杂系统，面向云服务（阿里云）、复杂应用（支付宝微信）或者Android操作系统设计的APM其也会有很大差别。因为系统的角色有差异，承载的业务逻辑有差异，存在的故障模式也有差异。下面将以AOSP为例，分析谷歌在其中设计的的功能与模块。 监控内容的范围：1.定义的故障模式事件，如应用无响应、应用崩溃等2.异常 如被捕获的Exception，硬件故障或调用超时3.Event 系统运行时的一些关键事件，如应用启动与关闭以及页面切换事件4.运行属性，如调用接口延时，调用接口次数，CPU使用率，IO，内存使用率等由于性能功耗等其他条件的限制我们只能在有限的接口设置监控，只能保存有限大小的数据。监控点的设置应当以KPI为牵引。多种不同的KPI能够方便的组合使用相同的监控点采集的数据。应当指出的是，我们监控的都是一种预定义的模式，而不是具体的故障。 场景（服务）监控：一个服务或者一个场景由一系列的步骤组成，任意步骤均可能失败，在失败时我们没法从内部观察这一点。例如对于亮屏这一操作，我们有一系列的过程日志，只有全部具备才说明任务完成。这样我们就可以在外部实时观测日志流，来了解任务完成的情况。这里提出场景监控的概念是因为业务与运行实体以及监控方可能不是同一主体，业务需要了解业务在整个系统中的运行情况。这就需要监控方提供一套通用的方法，让业务尽可能方便的使用。例如statsd中的MetricProducer就可以近似是一种场景监控的组件 监控发现：业务使用规定的模版或方法能够自动将自身添加到监控列表中，无需多余的配置。例如Watchdog。 分布式监控：这个概念在云服务上比较常见，例如Zabbix提供两种监控模式1.Proxy模式，纯事件日志收集转发，不做任何检查操作2.Node模式，具有完整的检查，预警以及上报功能，上报的对象是父节点。 与云服务场景不同，在终端侧进行监控需要更加考虑性能以及功耗的影响。所以应当尽可能的将处理以及分析移动到云服务侧。并且从运行实体上看，终端的监控有两个范畴1.在终端侧运行的服务 汇总所有的监控信息，进行预处理，对模式故障以及用户声称的故障进行上传 可以根据一系列的事件执行一些Action，例如告警合并，告警去重2.在云侧运行的服务，是对已经过滤过的数据进行二次的处理，可以主要部署统计信息 APM涉及的角色：开发人员： AOSP开发，包括各个层次的模块开发，如内核，驱动，服务等 内部应用开发 外部应用开发一般开发人员关注的是模块强相关的事件，包括1.当前以及历史版本模块的异常与日志2.单一故障或异常在版本间的变化3.接口调用事件，服务响应事件运营人员： 质量、服务、管理等运营人员关注与版本强相关的事件，包括1.版本上各个模块的故障与异常2.预定义的MTBF3.最终使用者的评价不同的角色有着不同的KPI，在使用APM的目的上也不相同。所以数据可视化要要有不同的逻辑。数据格式要简单，方便导出进而在不同的系统中进行分析。","categories":[],"tags":[]},{"title":"Mobile APMs","slug":"Mobile-APMs","date":"2019-02-11T15:21:04.000Z","updated":"2019-02-11T15:28:09.000Z","comments":true,"path":"2019/02/11/Mobile-APMs/","link":"","permalink":"http://yoursite.com/2019/02/11/Mobile-APMs/","excerpt":"","text":"APM： 国内：阿里：https://help.aliyun.com/document_detail/87906.html?spm=a2c4g.11186623.6.545.154a7d17zjSNFA腾讯：https://bugly.qq.com/v2/report百度：https://cloud.baidu.com/product/apm.html网易：http://apm.netease.com/携程：https://zhuanlan.zhihu.com/p/34371537?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=808827777729060864Others：https://help.cloudwise.com/help/19/26/336http://docs-mi.oneapm.com/function/Crash/Crash_index.htmlhttps://www.tingyun.com/tingyun_app.html 国外：https://mint.splunk.com/https://www.ibm.com/cloud-computing/cn-zh/learn-more/it-service-management/application-performance-management/https://www8.hp.com/us/en/software-solutions/apppulse-mobile-analytics-monitoring/https://www.microfocus.com/en-us/products/apppulse-mobile-app-apm-monitoring/overviewhttps://docs.appdynamics.com/display/PRO45/Crashes","categories":[],"tags":[]},{"title":"The generation of the backtrace","slug":"The-generation-of-the-backtrace","date":"2019-02-10T02:47:11.000Z","updated":"2019-02-20T16:12:27.000Z","comments":true,"path":"2019/02/10/The-generation-of-the-backtrace/","link":"","permalink":"http://yoursite.com/2019/02/10/The-generation-of-the-backtrace/","excerpt":"","text":"libbacktrace初始提交-https://android-review.googlesource.com/c/platform/system/core/+/66528debuggerd替换libbacktrace-https://android-review.googlesource.com/c/platform/system/core/+/66946rewrite libbacktrace in cpphttps://android-review.googlesource.com/c/platform/system/core/+/68667selinux 关联修改1https://android-review.googlesource.com/c/platform/external/sepolicy/+/68912","categories":[],"tags":[]},{"title":"Exception handling in different os","slug":"Exception-handling-in-different-os","date":"2019-02-09T09:02:08.000Z","updated":"2019-02-09T16:50:30.000Z","comments":true,"path":"2019/02/09/Exception-handling-in-different-os/","link":"","permalink":"http://yoursite.com/2019/02/09/Exception-handling-in-different-os/","excerpt":"","text":"AndroidAndroid 基于linux内核，使用 signal机制监听异常发生。值得一提的是，android 在自定义的libc库中(bionic)的linker初始化中会将debuggerd handler注册为默认的signal handler Kernel (linux)在cpu运行时发生错误，linux为这类错误注册了默认的异常处理程序，异常处理程序将执行：1.保存大部分寄存器的内容2.使用高级的C函数处理异常（向异常调用进程发送信号），在用户态处理异常（由于libc库中注册了默认处理函数，所以会在用户态处理函数）3.通过ret_from_exception()从异常处理程序中退出 Native ProcessNative进程直接使用signal机制进行异常处理，既可以使用默认的signal_handler，也可以使用自定义的。默认的signal handler将在自定义的处理完成之后继续执行。在此层开发应用的开发者可以使用signal来感知异常的发生。 Runtime Process取决于Runtime自己的实现ART会在启动时注册Signal SEGV，将由kernel产生的异常解释成Java的异常Java的异常会在调用链上抛出，直到被捕获，如果该异常在整个调用过程中没有被捕获会触发默认的异常处理函数，UncaughtExceptionHandler。虚拟机进程在API中提供setUncaughtException接口，为开发者提供VM级别的异常的感知。当然，VM Process通过jni依然能够使用signal机制来感知native的异常。 Excecption CollectorAndroid提供系统级的异常事件收集器，所有运行的进程在默认配置下最终会调用addErroToDropBox将异常日志保存只dropbox路径，以便统一打包。由于DropBoxManagerService属于AOSP，这里只包含了打包以及管理的部分， 并不包含上传云端的部分。本地使用firebase，云端使用android vitals Recovery API未提供异常恢复相关API ChromeOsChromeOs可以近似理解为在linux(非GUI)+chrome（GUI）。所以大部分的机制与Android类似，区别在于ChromeOs未为所有进程注册统一的默认的用户态异常处理程序。ChromeOs能够运行 1.Chrome App，运行在chrome的容器里，使用H5 Js以及css编写2.Android App，运行在Android Runtime for chrome(ARC)里3.linux原生App， Kernel (linux)ChromeOs与Android区别在于没有默认注册的用户态异常处理程序，默认的处理方式为1.coredump2.send coredump to crash reporter可以通过修改 /proc/sys/kernel/core_pattern 来修改默认的异常收集器 Native Process与 Android 类似 Runtime Process与 Android 类似 Excecption CollectorChromeOs使用crash reporter收集异常信息,产生统计信息，并定期上报云端。 Recovery API未提供异常恢复相关API 基于liunx的系统在kernel、Native Process、以及Vm Process上有着相似的实现方式，因为其都是基于Signal机制的。而在最终的异常事件收集器上有所差异。 MacOs(iOS)iOS以及MacOs在架构上类似，并且使用相同的内核XNU，这是一个混合内核。 XNU包含Mach(微内核)，bsd的实现是对Mach的封装。 Kernel (mach)在cpu运行时发生错误，会触发mach的异常处理程序exception_triage()，将异常转化为Mach消息。再调用exception_deliver()，将异常投递给thread、task以及host。kernel的异常可以通过set_exception_ports系列系统调用进行监听(阻塞) Native Process由于mach异常会通过bsd的包装再次以信号的形式发出，进程既可以直接使用kernel的方式阻塞监听mach的异常，也可以通过unix的信号机制，注册监听。 Runtime Process可以通过NSSetUncaughtExceptionHandler方式监听，实际上是对Signal监听的封装 Excecption CollectorReportCrash被用于收集上报设备中发生的Crash事件。Refs：https://help.apple.com/xcode/mac/current/#/dev675635e70设备使能日志上传云端使用App Store Connect Recovery APIRecovery Attempter(MacOs)Refs：https://developer.apple.com/library/archive/documentation/Cocoa/Conceptual/ErrorHandlingCocoa/RecoverFromErrors/RecoverFromErrors.html#//apple_ref/doc/uid/TP40001806-CH206-BCIDEGGF WindowsWindows同样使用混合内核(微内核)。 Kernel由于windows闭源，未向开发者提供kernel的异常处理接口。并且大部分的kernel异常会直接在kernel中处理。例如蓝屏 Native Process通过SetUnhandledExceptionFilter监听进程异常 Runtime Process使用Runtime的机制处理异常 Excecption CollectorWindows Error Reporting，在UnhandledExceptionFilter通知WER服务分析完异常之后，会通知用户处理异常。 Recovery APILast Known Good And SCM‘s FailureActions FuchsiaFuchsia使用名为Zircon的微内核，与darwin-xnu同样使用exception ports的概念来处理异常。任意thread、process以及jod均有自己的exception port Kernel可以通过zx_task_bind_exception_port将自定的handler用于处理目标对象产生的异常 Native Process可以通过zx_task_bind_exception_port将自定的handler用于处理目标对象产生的异常默认使用CrashService将Exception转发给用户态的crash analyzer处理 Runtime Process使用Runtime的机制处理异常Dart：unhandled_exception_callback转发给用户态的crash analyzer处理 Excecption Collectorcrash analyzer生成crash report。 Recovery API未提供异常恢复相关API","categories":[],"tags":[]},{"title":"Something about Crash reporter in different system","slug":"Something-about-Crash-reporter-in-different-system","date":"2019-01-13T06:57:22.000Z","updated":"2019-01-13T14:44:06.000Z","comments":true,"path":"2019/01/13/Something-about-Crash-reporter-in-different-system/","link":"","permalink":"http://yoursite.com/2019/01/13/Something-about-Crash-reporter-in-different-system/","excerpt":"","text":"通用这里所描述的都指的是用户态的crash，kernel的crash的处理方式略有区别。本文只是简述crash信息保存的流程。 在linux系统上，应用异常是通过信号从内核通知上来的，在用户态注册的信号处理函数能够保存一些信息，在信号处理函数运行完成之后，kernel可能会进行一些core dump，这取决于配置。 当用户态crash时，信号传递给预先注册的signal handler，这时crash尚处于active状态，signalhandler运行在同一个进程里，此时也可以通知外部的进程，例如tombstoned，进行dump操作。 大致需要保存的信息主要来源于1.procfs 2.ptrace 3.logs 4.tracesprocfs在进程尚未退出之前都会存在，可以从中保存maps、cmd line、 stat等信息ptrace系统调用能够查看被监听进程的内存、寄存器.logs包括 dev/kmsg 的 kernel日志 Android在Android上使用 debuggerd tombstoned以及crash_dump进行crash事件日志的收集。tombstoned是一个常驻的daemon，用于crash事件的处理，crash_dump在crash发生时启动，dump其stack以及register信息，debuggerd充当连接两者的作用。由于java crash与native crash的处理流程略有差异，这里只介绍native crash的流程主要流程：1.bionic库中的linker初始化过程中进行debuggerd客户端的初始化2.在crash发生时运行预先注册的信号处理函数，设置相关flag，clone 一个进程进行dump，原进程等待dump结束。3.子进程中运行crash_dump,会fork出新进程，从clone的进程中读取所需的数据.5.crash_dump将数据写入tombstoned(g_output_fd) 在dump完成后会通知6.tombstoned生成tombstone文件 Chromium OS‎Chromium OS‎ 也是基于linux内核，所以应该有着相似的机制.所有crash的异常从kernel触发，但Chromium os并未使用signal返回到用户态进行处理，而是1.进行coredump，并将crash进程信息连同coredump一同发送给crash_reporter 这时kernel 2.6.19的新增机制，使用| (pipe symbol) 将core dumps发送到用户进程 refs： http://man7.org/linux/man-pages/man5/core.5.html2.当然，应用也可以使用signal机制进行触发，一般使用的是谷歌的breakpad或者crashpad进行收集 这种机制类似与Android的机制，只是信号处理者从debuggerd的hanlder换成了breakapd3.crash_reporter coredump转换成minidump，并定期上传4.如果应用自行注册信号处理，并在处理完成之后退出，则不会产生core dump信息，也不会发送给crash_reporter FuchsiaFuchsia 使用名为zircon的内核，所以并不使用signal作为crash的触发源。运行实体也略有差异。对于一个内核对象，总是会有一个Exception Port，用于处理该对象的异常。这个对象可以是线程，进程抑或是一个任务。一个用户态的对象能够通过bind的方式对另一个对象的异常进行监听。这个过程由系统调用完成-zx_task_bind_exception_portrefs:https://fuchsia.googlesource.com/zircon/+/master/docs/exceptions.md所以可以在一个用户态进程里bind所有的进程或者线程异常？ 从提交记录来看Google 的CrashPad是支持fuchsia的refs:https://chromium-review.googlesource.com/q/+fuchsia+project:crashpad/crashpad,225可以考虑从其实现中查看一般的crash记录流程。","categories":[],"tags":[]},{"title":"Statsd In android 9(3)","slug":"Statsd-In-android-9-3","date":"2019-01-02T14:36:35.000Z","updated":"2019-01-02T15:48:51.000Z","comments":true,"path":"2019/01/02/Statsd-In-android-9-3/","link":"","permalink":"http://yoursite.com/2019/01/02/Statsd-In-android-9-3/","excerpt":"","text":"前面已经介绍了statsd的接口以及调用流程，本篇介绍接口自动生成的代码。 ###StatsLogInternalframework/base/core仓中的StatsLogInternal.java会被编译到framework.jar中，因而会在编译该jar包前生成。查看framework/base/Android.bp 1234567891011121314java_library &#123; name: &quot;framework&quot;, srcs: [ ... &quot;:framework-statslog-gen&quot;,],genrule &#123; name: &quot;framework-statslog-gen&quot;, tools: [&quot;stats-log-api-gen&quot;], cmd: &quot;$(location stats-log-api-gen) --java $(out)&quot;, out: [&quot;android/util/StatsLogInternal.java&quot;],&#125; 可以理解为执行stats-log-api-gen这个工具，生成StatsLogInternal.java这个文件-查看framework/base/tools/stats_log_api_gen/Android.bp首先生成 stats-log-api-gen，再生成 statslog.h 以及 statslog.cpp，最后生成 libstatslog 123456789101112131415 name: &quot;statslog.h&quot;, tools: [&quot;stats-log-api-gen&quot;], cmd: &quot;$(location stats-log-api-gen) --header $(genDir)/statslog.h&quot;, out: [ &quot;statslog.h&quot;, ],&#125;genrule &#123; name: &quot;statslog.cpp&quot;, tools: [&quot;stats-log-api-gen&quot;], cmd: &quot;$(location stats-log-api-gen) --cpp $(genDir)/statslog.cpp&quot;, out: [ &quot;statslog.cpp&quot;, ],&#125; 这里以java文件生成为例，查看代码的生成流程-12345678FILE* out = fopen(javaFilename.c_str(), &quot;w&quot;);if (out == NULL) &#123; fprintf(stderr, &quot;Unable to open file for write: %s\\n&quot;, javaFilename.c_str()); return 1;&#125;errorCount = android::stats_log_api_gen::write_stats_log_java( out, atoms, attributionDecl);fclose(out); 0.解析proto文件中的定义的常量(atoms)以及方法的参数(attributionDecl, attributionSignature)解析atoms的方法均在Collation.cpp中定义1.写入公用的字段，如import 以及类名2.写入atom的常量3.写入atom枚举的常量4.写入write以及write_non_chained代码123write_java_method(out, &quot;write&quot;, atoms.signatures, attributionDecl);write_java_method(out, &quot;write_non_chained&quot;, atoms.non_chained_signatures, attributionDecl);write_java_work_source_method(out, atoms.signatures); 整个过程很简单，这里值得借鉴的是动态根据配置批量生成接口的方法具体来说可以参考bp文件中的genrule的定义以及使用-","categories":[],"tags":[]},{"title":"Statsd In android 9 (2)","slug":"Statsd-In-android-9-2","date":"2018-12-19T15:09:27.000Z","updated":"2018-12-21T15:04:46.000Z","comments":true,"path":"2018/12/19/Statsd-In-android-9-2/","link":"","permalink":"http://yoursite.com/2018/12/19/Statsd-In-android-9-2/","excerpt":"","text":"前面一部分已经介绍了statsd上报使用的两个接口Java侧使用StatsLogNative侧使用android::util::stats_write最终都通过sock写入statsd中，下面将先介绍statsd的daemon部分的结构，随后介绍事件的管理。 statsd位于frameworks/base/cmds/statsd路径下其目录结构：123456789101112131415161718statsd----benchmark----src--------anomaly \\\\异常事件触法跟踪管理--------condition \\\\根据当前条件触法新的事件--------config \\\\配置管理--------external \\\\事件拉取管理--------guardrail \\\\事件数量管理--------logd \\\\循环读取日志事件并分发--------matchers \\\\事件匹配，一类Matcher跟踪一类事件--------metrics \\\\事件匹配计算，将计算结果保存或者上报--------packages \\\\缓存应用信息，包含包名以及版本号--------perfetto \\\\数据源配置描述--------socket \\\\日志sock监听--------storage \\\\日志文件写入，配置读取--------subscriber \\\\事件订阅通知----tests \\\\单元测试----tools \\\\功能测试 那么还是先从main函数看该服务的初始化有一下几个步骤：1.启动Looper2.配置Binder3.与Java服务通信，确认启动4.根据配置看是从logd读取事件还是从socket中读取事件，前面一部分也看到了在日志上报时，具体写入到logd或者statsd也是由配置控制的5.循环从looper中读取事件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950int main(int /*argc*/, char** /*argv*/) &#123; // Set up the looper sp&lt;Looper&gt; looper(Looper::prepare(0 /* opts */)); // Set up the binder sp&lt;ProcessState&gt; ps(ProcessState::self()); ps-&gt;setThreadPoolMaxThreadCount(9); ps-&gt;startThreadPool(); ps-&gt;giveThreadPoolName(); IPCThreadState::self()-&gt;disableBackgroundScheduling(true); // Create the service sp&lt;StatsService&gt; service = new StatsService(looper); if (defaultServiceManager()-&gt;addService(String16(&quot;stats&quot;), service) != 0) &#123; ALOGE(&quot;Failed to add service&quot;); return -1; &#125; service-&gt;sayHiToStatsCompanion(); service-&gt;Startup(); sp&lt;StatsSocketListener&gt; socketListener = new StatsSocketListener(service); if (kUseLogd) &#123; ALOGI(&quot;using logd&quot;); // Start the log reader thread status_t err = start_log_reader_thread(service); if (err != NO_ERROR) &#123; return 1; &#125; &#125; if (kUseStatsdSocket) &#123; ALOGI(&quot;using statsd socket&quot;); // Backlog and /proc/sys/net/unix/max_dgram_qlen set to large value if (socketListener-&gt;startListener(600)) &#123; exit(1); &#125; &#125; // Loop forever -- the reports run on this thread in a handler, and the // binder calls remain responsive in their pool of one thread. while (true) &#123; looper-&gt;pollAll(-1 /* timeoutMillis */); &#125; ALOGW(&quot;statsd escaped from its loop.&quot;); return 1;&#125; 下面以socket日志读取为例子来观察日志的流向主线程启动后开始不断从socket中读取事件，并把日志事件发送给服务具体来说就是1.创建一块足够大小的buffer2.从socket中读取事件到构造好的内存结构中（这块暂时还没看明白- -3.校验头部4.包装成Event事件5.调用服务的OnLogEvent处理 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768bool StatsSocketListener::onDataAvailable(SocketClient* cli) &#123; static bool name_set; if (!name_set) &#123; prctl(PR_SET_NAME, &quot;statsd.writer&quot;); name_set = true; &#125; // + 1 to ensure null terminator if MAX_PAYLOAD buffer is received char buffer[sizeof_log_id_t + sizeof(uint16_t) + sizeof(log_time) + LOGGER_ENTRY_MAX_PAYLOAD + 1]; struct iovec iov = &#123;buffer, sizeof(buffer) - 1&#125;; alignas(4) char control[CMSG_SPACE(sizeof(struct ucred))]; struct msghdr hdr = &#123; NULL, 0, &amp;iov, 1, control, sizeof(control), 0, &#125;; int socket = cli-&gt;getSocket(); // To clear the entire buffer is secure/safe, but this contributes to 1.68% // overhead under logging load. We are safe because we check counts, but // still need to clear null terminator // memset(buffer, 0, sizeof(buffer)); ssize_t n = recvmsg(socket, &amp;hdr, 0); if (n &lt;= (ssize_t)(sizeof(android_log_header_t))) &#123; return false; &#125; buffer[n] = 0; struct ucred* cred = NULL; struct cmsghdr* cmsg = CMSG_FIRSTHDR(&amp;hdr); while (cmsg != NULL) &#123; if (cmsg-&gt;cmsg_level == SOL_SOCKET &amp;&amp; cmsg-&gt;cmsg_type == SCM_CREDENTIALS) &#123; cred = (struct ucred*)CMSG_DATA(cmsg); break; &#125; cmsg = CMSG_NXTHDR(&amp;hdr, cmsg); &#125; struct ucred fake_cred; if (cred == NULL) &#123; cred = &amp;fake_cred; cred-&gt;pid = 0; cred-&gt;uid = DEFAULT_OVERFLOWUID; &#125; char* ptr = ((char*)buffer) + sizeof(android_log_header_t); n -= sizeof(android_log_header_t); log_msg msg; msg.entry.len = n; msg.entry.hdr_size = kLogMsgHeaderSize; msg.entry.sec = time(nullptr); msg.entry.pid = cred-&gt;pid; msg.entry.uid = cred-&gt;uid; memcpy(msg.buf + kLogMsgHeaderSize, ptr, n + 1); LogEvent event(msg); // Call the listener mListener-&gt;OnLogEvent(&amp;event, false /*reconnected, N/A in statsd socket*/); return true;&#125; 服务的定义位于头文件中，他实现了LogListener的接口而在OnLogEvent的实现中实际调用的是mProcessor的OnLogEvent方法12345678910111213141516171819202122class StatsService : public BnStatsManager, public LogListener, public IBinder::DeathRecipient--------void StatsService::OnLogEvent(LogEvent* event, bool reconnectionStarts) &#123; mProcessor-&gt;OnLogEvent(event, reconnectionStarts);&#125;--------mProcessor = new StatsLogProcessor(mUidMap, mAnomalyAlarmMonitor, mPeriodicAlarmMonitor, getElapsedRealtimeNs(), [this](const ConfigKey&amp; key) &#123; sp&lt;IStatsCompanionService&gt; sc = getStatsCompanionService(); auto receiver = mConfigManager-&gt;GetConfigReceiver(key); if (sc == nullptr) &#123; VLOG(&quot;Could not find StatsCompanionService&quot;); return false; &#125; else if (receiver == nullptr) &#123; VLOG(&quot;Statscompanion could not find a broadcast receiver for %s&quot;, key.ToString().c_str()); return false; &#125; else &#123; sc-&gt;sendDataBroadcast(receiver, mProcessor-&gt;getLastReportTimeNs(key)); return true; &#125;&#125; mProcessor的对象实际是一个StatsLogProcessor，他是整个日志处理的核心其实现了ConfigListener，可以动态更新配置 1234567891011121314151617181920212223242526272829303132333435363738&#123;StatsLogProcessor(const sp&lt;UidMap&gt;&amp; uidMap, const sp&lt;AlarmMonitor&gt;&amp; anomalyAlarmMonitor, const sp&lt;AlarmMonitor&gt;&amp; subscriberTriggerAlarmMonitor, const int64_t timeBaseNs, const std::function&lt;bool(const ConfigKey&amp;)&gt;&amp; sendBroadcast);virtual ~StatsLogProcessor();void OnLogEvent(LogEvent* event, bool reconnectionStarts);// for testing only.void OnLogEvent(LogEvent* event);void OnConfigUpdated(const int64_t timestampNs, const ConfigKey&amp; key, const StatsdConfig&amp; config);void OnConfigRemoved(const ConfigKey&amp; key);size_t GetMetricsSize(const ConfigKey&amp; key) const;void onDumpReport(const ConfigKey&amp; key, const int64_t dumpTimeNs, const bool include_current_partial_bucket, const DumpReportReason dumpReportReason, vector&lt;uint8_t&gt;* outData);/* Tells MetricsManager that the alarms in alarmSet have fired. Modifies anomaly alarmSet. */void onAnomalyAlarmFired( const int64_t&amp; timestampNs, unordered_set&lt;sp&lt;const InternalAlarm&gt;, SpHash&lt;InternalAlarm&gt;&gt; alarmSet);/* Tells MetricsManager that the alarms in alarmSet have fired. Modifies periodic alarmSet. */void onPeriodicAlarmFired( const int64_t&amp; timestampNs, unordered_set&lt;sp&lt;const InternalAlarm&gt;, SpHash&lt;InternalAlarm&gt;&gt; alarmSet);/* Flushes data to disk. Data on memory will be gone after written to disk. */void WriteDataToDisk(const DumpReportReason dumpReportReason);// Reset all configs.void resetConfigs();&#125; OnLogEvent里具体的流程如下：1.检查是否是重连，缓存中会保存最后一个事件，避免丢失2.如果是重连检查事件是否已经处理过了，如果处理过了则返回，若没处理过记录重连，继续处理3.处理特殊的事件4.清理Pull事件缓存5.将事件交由mMetricsManagers处理6.刷新事件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576&#123;#ifdef VERY_VERBOSE_PRINTING if (mPrintAllLogs) &#123; ALOGI(&quot;%s&quot;, event-&gt;ToString().c_str()); &#125;#endif const int64_t currentTimestampNs = event-&gt;GetElapsedTimestampNs(); if (reconnected &amp;&amp; mLastTimestampSeen != 0) &#123; // LogReader tells us the connection has just been reset. Now we need // to enter reconnection state to find the last CP. mInReconnection = true; &#125; if (mInReconnection) &#123; // We see the checkpoint if (currentTimestampNs == mLastTimestampSeen) &#123; mInReconnection = false; // Found the CP. ignore this event, and we will start to read from next event. return; &#125; if (currentTimestampNs &gt; mLargestTimestampSeen) &#123; // We see a new log but CP has not been found yet. Give up now. mLogLossCount++; mInReconnection = false; StatsdStats::getInstance().noteLogLost(currentTimestampNs); // Persist the data before we reset. Do we want this? WriteDataToDiskLocked(CONFIG_RESET); // We see fresher event before we see the checkpoint. We might have lost data. // The best we can do is to reset. resetConfigsLocked(currentTimestampNs); &#125; else &#123; // Still in search of the CP. Keep going. return; &#125; &#125; mLogCount++; mLastTimestampSeen = currentTimestampNs; if (mLargestTimestampSeen &lt; currentTimestampNs) &#123; mLargestTimestampSeen = currentTimestampNs; &#125; resetIfConfigTtlExpiredLocked(currentTimestampNs); StatsdStats::getInstance().noteAtomLogged( event-&gt;GetTagId(), event-&gt;GetElapsedTimestampNs() / NS_PER_SEC); // Hard-coded logic to update the isolated uid&apos;s in the uid-map. // The field numbers need to be currently updated by hand with atoms.proto if (event-&gt;GetTagId() == android::util::ISOLATED_UID_CHANGED) &#123; onIsolatedUidChangedEventLocked(*event); &#125; if (mMetricsManagers.empty()) &#123; return; &#125; int64_t curTimeSec = getElapsedRealtimeSec(); if (curTimeSec - mLastPullerCacheClearTimeSec &gt; StatsdStats::kPullerCacheClearIntervalSec) &#123; mStatsPullerManager.ClearPullerCacheIfNecessary(curTimeSec * NS_PER_SEC); mLastPullerCacheClearTimeSec = curTimeSec; &#125; if (event-&gt;GetTagId() != android::util::ISOLATED_UID_CHANGED) &#123; // Map the isolated uid to host uid if necessary. mapIsolatedUidToHostUidIfNecessaryLocked(event); &#125; // pass the event to metrics managers. for (auto&amp; pair : mMetricsManagers) &#123; pair.second-&gt;onLogEvent(*event); flushIfNecessaryLocked(event-&gt;GetElapsedTimestampNs(), pair.first, *(pair.second)); &#125;&#125; mMetricsManagers的创建是在读取配置的时候每个MetricsManager可以监听一组特定的进程123456789101112131415161718192021void StatsLogProcessor::OnConfigUpdatedLocked( const int64_t timestampNs, const ConfigKey&amp; key, const StatsdConfig&amp; config) &#123; VLOG(&quot;Updated configuration for key %s&quot;, key.ToString().c_str()); sp&lt;MetricsManager&gt; newMetricsManager = new MetricsManager(key, config, mTimeBaseNs, timestampNs, mUidMap, mAnomalyAlarmMonitor, mPeriodicAlarmMonitor); if (newMetricsManager-&gt;isConfigValid()) &#123; mUidMap-&gt;OnConfigUpdated(key); if (newMetricsManager-&gt;shouldAddUidMapListener()) &#123; // We have to add listener after the MetricsManager is constructed because it&apos;s // not safe to create wp or sp from this pointer inside its constructor. mUidMap-&gt;addListener(newMetricsManager.get()); &#125; newMetricsManager-&gt;refreshTtl(timestampNs); mMetricsManagers[key] = newMetricsManager; VLOG(&quot;StatsdConfig valid&quot;); &#125; else &#123; // If there is any error in the config, don&apos;t use it. ALOGE(&quot;StatsdConfig NOT valid&quot;); &#125;&#125; MetricsManager的结构：里面持有了std::vector","categories":[],"tags":[]},{"title":"Statsd In android 9 （1）","slug":"Statsd-In-android-9","date":"2018-12-15T13:39:17.000Z","updated":"2018-12-20T14:44:07.000Z","comments":true,"path":"2018/12/15/Statsd-In-android-9/","link":"","permalink":"http://yoursite.com/2018/12/15/Statsd-In-android-9/","excerpt":"","text":"android系统中有很多不同功能的日志，如dumpsys dumpstate。anr以及crash时候也有单点的日志。然而，一些系统的问题，如性能功耗以及稳定性问题是由于不明确的单点的缺陷或者故障扩散导致的。仅靠最后问题发生时的日志有时难以定位问题，还有一些问题，例如黑屏，原因有很多种，所以能否将可能相关的事件汇聚到一处，做数据分析也会方便一些。大概基于这种目的，android p版本中新增一种日志上报方式，接口位于：1android.util.StatsLog 新增一个用于处理统计数据的常驻进程 statsdstatsd 与logd一样使用sock收集日志所有日志上报函数均直接使用入参写入sock，没有中间数据接口statsd的收集在java以及native均有涉及，包含pull 以及 push两个模式 下面部分代码引用自 aosp android-9.0.0_r22 StatsLog接口类StatsLog类继承于StatsLogInternal，而StatsLogInternal是在编译期由protobuf自动生成的，主要是一些常量以及数据结构的定义。StatsLog类中主要有3个接口,是通过binder调用到服务端：123public static boolean logStart(int label) //开始记录过程类事件public static boolean logStop(int label) //停止记录过程类事件public static boolean logEvent(int label) //记录结果类事件 还有从Internal类里继承而来的native方法，framework中上报的方法使用的是write，以及 write_non_chained123public static native int write(int code);... ...public static native int write_non_chained(int code, int arg1, java.lang.String arg2, int arg3); 其他都是同名的重载函数，最终调用的函数位于： out/soong/.intermediates/frameworks/base/tools/stats_log_api_gen/statslog.h/gen/statslog.hsystem/core/libstats/include/stats_event_list.hstats_event_list.write123456789101112131415161718try_stats_write(int32_t code, char const* arg1, int64_t arg2)&#123; if (kStatsdEnabled) &#123; stats_event_list event(kStatsEventTag); event &lt;&lt; android::elapsedRealtimeNano(); event &lt;&lt; code; if (arg1 == NULL) &#123; arg1 = &quot;&quot;; &#125; event &lt;&lt; arg1; event &lt;&lt; arg2; return event.write(LOG_ID_STATS); &#125; else &#123; return 1; &#125;&#125; stats_event_list创建一个以事件为id的logger，并把信息写入，缓存，最后使用write_to_logger，将android_log_context写入logger。123456789101112int write(log_id_t id = LOG_ID_EVENTS) &#123; // facilitate -EBUSY retry if ((ret == -EBUSY) || (ret &gt; 0)) &#123; ret = 0; &#125; int retval = write_to_logger(ctx, id); // existing errors trump transmission errors if (!ret) &#123; ret = retval; &#125; return ret;&#125; 一开始以为通过binder调用写入事件，这里看起来为了提高性能使用的还是native的方法， 并且通过直接用函数入参的方式来减少结构封包解包的性能开销。由于没有封包的过程，造成了参数不同的重载函数很多，这里使用自动生成的方式生成了很多函数。所以这几个类都在 out/soong/.intermediates/里。 systlem/core/libstats/stats_event_list.h12345678910111213141516171819int write_to_logger(android_log_context ctx, log_id_t id) &#123; int retValue = 0; if (WRITE_TO_LOGD) &#123; retValue = android_log_write_list(ctx, id); &#125; if (WRITE_TO_STATSD) &#123; // log_event_list&apos;s cast operator is overloaded. int ret = stats_write_list(ctx); // In debugging phase, we may write to both logd and statsd. Prefer to // return statsd socket write error code here. if (ret &lt; 0) &#123; retValue = ret; &#125; &#125; return retValue;&#125; 最后可以选择是直接将事件输出到 logd中还是输出到statsd中。当然，无论是logd或是statsd都是通过sock写入的。 statsd服务statsd 服务分为两个部分，一部分是java的服务代理，用于在java侧提供接口，并注册一些服务监听。如应用安装更新等。 StatsCompanionService.java123traceBeginAndSlog(&quot;StartStatsCompanionService&quot;);mSystemServiceManager.startService(StatsCompanionService.Lifecycle.class);traceEnd(); 另一部分运行在native的statsd中。 frameworks/base/cmds/statsd/src/main.cpp1234567891011121314151617181920212223242526272829303132333435363738394041// Set up the bindersp&lt;ProcessState&gt; ps(ProcessState::self());ps-&gt;setThreadPoolMaxThreadCount(9);ps-&gt;startThreadPool();ps-&gt;giveThreadPoolName();IPCThreadState::self()-&gt;disableBackgroundScheduling(true);// Create the servicesp&lt;StatsService&gt; service = new StatsService(looper);if (defaultServiceManager()-&gt;addService(String16(&quot;stats&quot;), service) != 0) &#123; ALOGE(&quot;Failed to add service&quot;); return -1;&#125;service-&gt;sayHiToStatsCompanion();service-&gt;Startup();sp&lt;StatsSocketListener&gt; socketListener = new StatsSocketListener(service);if (kUseLogd) &#123; ALOGI(&quot;using logd&quot;); // Start the log reader thread status_t err = start_log_reader_thread(service); if (err != NO_ERROR) &#123; return 1; &#125;&#125;if (kUseStatsdSocket) &#123; ALOGI(&quot;using statsd socket&quot;); // Backlog and /proc/sys/net/unix/max_dgram_qlen set to large value if (socketListener-&gt;startListener(600)) &#123; exit(1); &#125;&#125;// Loop forever -- the reports run on this thread in a handler, and the// binder calls remain responsive in their pool of one thread.while (true) &#123; looper-&gt;pollAll(-1);&#125;","categories":[],"tags":[]},{"title":"Android Reliability","slug":"关于Android稳定性的一些思考","date":"2018-11-20T14:52:34.000Z","updated":"2018-11-20T14:52:34.000Z","comments":true,"path":"2018/11/20/关于Android稳定性的一些思考/","link":"","permalink":"http://yoursite.com/2018/11/20/关于Android稳定性的一些思考/","excerpt":"","text":"稳定 顾名思义 指的是系统以及应用正常工作的状态，但由于各种原因系统或者应用工作在不正常的状态，例如应用卡顿，闪退，系统卡死黑屏乃至重启。关于稳定性工作，一般也就分成两个方面 1.定位增强–如何能够尽快正向解决问题 2.容错恢复设计–在问题发生时如何能尽量减少异常的时间这两个方面在原生系统（谷歌aosp）中已经有着不少体现。并且这两个方面应该说是相辅相成的。如在应用anr时会有anr日志，同时提示用户等待或者停止应用，再如watchdog时会先抓去watchdog日志，并重启虚拟机。从大部分问题发生的原因来看主要有以下几个问题： 1.异常场景判断不够 （如并发访问 2.设计时序错误 （如环形依赖的服务 3.编码规范 （如判空、fd有效判断如何做好稳定性工作？ 1.前端上代码静态检查，流程上代码review，设计上进行多方考虑。 2.问题发生时的现场抓取能力构建。 任何问题的检测都是有开销的，理想的场景是在内部用户测试时尽量抓取全的日志，而在商用上关闭所有的日志。 可以考虑的是一直抓取日志，并打包。在问题发生时提取时间点前后一段时间的日志。 重新定义一套日志标签，用于提取问题的发生时的特征，用户上报问题的现象作为分类的结果- 利用专家知识，对日志做初步的筛选，分类，提高开发定位效率。 除了流水日志外，各种栈或者dumpsys的抓取时机也十分重要。能否在正确的时间抓取也是需要考量的地方-常用的一些日志： 由于底层是linux大部分日志或者系统状态都能够通过sys/fs 或者 proc节点下获得。 framework层有各种dumpsys，并且还有 app日志以及event日志","categories":[],"tags":[]},{"title":"Decentralization in Bitcoin","slug":"Decentralization-in-Bitcoin","date":"2018-02-06T10:06:36.000Z","updated":"2018-02-06T12:01:27.000Z","comments":true,"path":"2018/02/06/Decentralization-in-Bitcoin/","link":"","permalink":"http://yoursite.com/2018/02/06/Decentralization-in-Bitcoin/","excerpt":"","text":"Problems to be solved Who maintains the ledger of transactions? Who has authority over which transactions are valid? Who creates new bitcoins? General Distributed consensusThere are n nodes that each have an input value. Some of these nodes are faulty or malicious. A distributed consensus protocol has the following two properties:● It must terminate with all honest nodes in agreement on the value● The value must have been generated by an honest node Imperfect Conditions in systemFirstly, consensus in general is a hard problem since nodes might crash or be outright malicious.Secondly, and specifically in the Bitcoin context, the network is highly imperfect. It’s a peer‐to‐peer system, and not all pairs of nodes are connected to each other.Finally, there’s a lot of latency in the system because it’s distributed all over the Internet. How blockchain achieved consensusFirst, it introduces the idea of incentives, which is novel for a distributed consensus protocol.Second, Bitcoin embraces the notion of randomness. Bitcoin consensus algorithm (simplified) New transactions are broadcast to all nodes Each node collects new transactions into a block In each round a r andom node gets to broadcast its block Other nodes accept the block only if all transactions in it are valid (unspent, valid signatures) Nodes express their acceptance of the block by including its hash in the next block they create Double‐spend attack In fact, the double‐spend probability decreases exponentially with the number of confirmations. So, if the transaction that you’re interested in has received k confirmations, then the probability that a double‐spend transaction will end up on the long‐term consensus chain goes down exponentially as a function of k . The most common heuristic that’s used in the Bitcoin ecosystem is to wait for six confirmations. Incentives and proof of work Block RewardAccording to the rules of Bitcoin, the node that creates a block gets to include a special transaction in that block. This transaction is a coin‐creation transaction Transaction feesSo if you’re a node that’s creating a block that contains, say, 200 transactions, then the sum of all those 200 transaction fees is paid to the address that you put into that block. Mining and proof‐of‐workThe key idea behind proof‐of‐work is that we approximate the selection of a random node by instead selecting nodes in proportion to a resource that we hope that nobody can monopolize.If, for example, that resource is computing power, then it’s a proof‐of‐work system. Alternately, it could be in proportion to ownership of the currency, and that’s called p roof‐of‐stake. Although it’s not used in Bitcoin, proof‐of‐stake is a legitimate alternate model and it’s used in other cryptocurrencies. Bitcoin achieves proof‐of‐work using hash puzzles1H(nonce || prev_hash || tx || tx || ... || tx) &lt; target","categories":[],"tags":[{"name":"Blockchain","slug":"Blockchain","permalink":"http://yoursite.com/tags/Blockchain/"}]},{"title":"simple cryptography tools in blockchain","slug":"simple-cryptography-in-blockchain","date":"2018-02-04T14:17:09.000Z","updated":"2018-02-06T10:12:49.000Z","comments":true,"path":"2018/02/04/simple-cryptography-in-blockchain/","link":"","permalink":"http://yoursite.com/2018/02/04/simple-cryptography-in-blockchain/","excerpt":"","text":"Hash Functiontakes any string as inputfixed-sized output(usually 256 bits)efficiently computable security properties:collision-freehidingpuzzle-friendly SHA-256 Hash Pointer Merkle Tree Digital signature schemeOnly you can sign, but anyone can verify (sk, pk) := generateKeys(keysize) sig := sign(sk, message) isValid := verify(pk, message, sig)Public keys as IdentitiesA useful trick as Bitcoin do A Simple Cryptocurrency Example GoofyCoin1.Goofy, can create new coins whenever he wants and these newly created coins belong to him2.whoever owns a coin can transfer it on to someone elsedouble‐spending attack — Alice is spending the same coin twice. ScroogeCoinScrooge publishes an append‐only ledger containing the history of all the transactions that have happened.ScroogeCoin block chain.CreateCoins transaction.A PayCoins Transaction.","categories":[],"tags":[{"name":"Blockchain","slug":"Blockchain","permalink":"http://yoursite.com/tags/Blockchain/"}]},{"title":"从Systrace学习binder kernel 调用过程","slug":"Some-thing-about-binder-in-kernel","date":"2017-05-30T11:39:51.000Z","updated":"2018-02-06T13:04:39.000Z","comments":true,"path":"2017/05/30/Some-thing-about-binder-in-kernel/","link":"","permalink":"http://yoursite.com/2017/05/30/Some-thing-about-binder-in-kernel/","excerpt":"","text":"一.Systrace简介从目前使用看来，Systrace主要包含两个部分：1.手机中Systrace日志打印框架2.Chrome或其他日志解析工具，能够将上述生成的日志解析成可视化的图形其精度可以达到ns 每年的GoogleI/O好像都有提及这个方便的工具，具体的用法的官方教程如下：视频：https://www.youtube.com/watch?v=Qfo5fdoXrTU&amp;index=10&amp;list=PLWz5rJ2EKKc-odHd6XEaf7ykfsosYyCKp文字：https://developer.android.com/studio/profile/systrace.html 二.binder kernel driverbinder是android中ipc通信的主要机制其原理可以参考gityuan的博客http://gityuan.com/2015/11/01/binder-driver/http://gityuan.com/2015/11/02/binder-driver-2/ 三.Systrace in binder kernelSystrace 作为android 性能调优的主要手段在源码中随处可见如 AMS 中的 Trace.beginTrace又如 InputDispatcher 中的 ATRACE 想要抓取binder kernel的trace 首先要打开开关其路径如下：/sys/kernel/debug/tracing/events/binder/我们需要将其 enable 置为 1echo 1 &gt; enable 随后我们可以使用脚本或者android device monitor 抓取 binder kernel driver的日志 一个简单的调用过程如下其中6834 为 nfc服务的 pid，3412 为 system_servernfc服务向 ams 发起 binder通信 1.binder 驱动收到 BC_TRANSACTION 的通信请求 创建binder_transaction 结构体，分配内存，并将通信的数据写入内存 2.binder 驱动唤醒 system_server处理请求，使用 BR_TRANSACTION 的命令作为标识 system_server从队列中读取通信请求，并将结果写入内存。 3.system_server 发起 BC_REPLY 请求，该请求是 ONEWAY 的 binder驱动为新的请求分配内存，并将任务加入到目标进程的队列中 4.binder 驱动告知请求端 请求已经完成 BR_TRANSACTION_COMPLETE,并释放内存 5.binder驱动 从任务队列中读取前面 BC_REPLY 的通信任务，向服务端发起 BR_TRANSACTION_COMPLETE的请求 6.请求端处理 BR_REPLY 的请求需要注意的是每个cmd工作在那个进程中，以及其前后的调用次序 Systrace 日志：123456789101112131415161718192021222324252627com.android.nfc-6834 ( 6834) [004] ...2 1005.674370: binder_command: cmd=0x40406300 BC_TRANSACTIONcom.android.nfc-6834 ( 6834) [004] ...2 1005.674374: binder_transaction: transaction=141734 dest_node=21476 dest_proc=3412 dest_thread=0 reply=0 flags=0x10 code=0x15com.android.nfc-6834 ( 6834) [004] ...2 1005.674375: binder_transaction_alloc_buf: transaction=141734 data_size=240 offsets_size=0com.android.nfc-6834 ( 6834) [004] .N.2 1005.674385: binder_write_done: ret=0 Binder:3412_8-4646 ( 3412) [004] ...2 1005.674399: binder_transaction_received: transaction=141734 Binder:3412_8-4646 ( 3412) [004] ...2 1005.674400: binder_return: cmd=0x80407202 BR_TRANSACTION Binder:3412_8-4646 ( 3412) [004] ...2 1005.674401: binder_read_done: ret=0 Binder:3412_8-4646 ( 3412) [004] ...1 1005.674403: binder_ioctl_done: ret=0 Binder:3412_8-4646 ( 3412) [004] ...1 1005.674621: binder_ioctl: cmd=0xc0306201 arg=0x7be9dd1fa8 Binder:3412_8-4646 ( 3412) [004] ...2 1005.674623: binder_command: cmd=0x40406301 BC_REPLY Binder:3412_8-4646 ( 3412) [004] ...2 1005.674625: binder_transaction: transaction=141735 dest_node=0 dest_proc=6834 dest_thread=6834 reply=1 flags=0x0 code=0x0 Binder:3412_8-4646 ( 3412) [004] ...2 1005.674627: binder_transaction_alloc_buf: transaction=141735 data_size=328 offsets_size=8 Binder:3412_8-4646 ( 3412) [004] ...2 1005.674633: binder_transaction_fd: transaction=141735 src_fd=276 ==&gt; dest_fd=35 Binder:3412_8-4646 ( 3412) [004] ...2 1005.674634: binder_write_done: ret=0 Binder:3412_8-4646 ( 3412) [004] ...1 1005.674635: binder_wait_for_work: proc_work=0 transaction_stack=0 thread_todo=1 Binder:3412_8-4646 ( 3412) [004] ...2 1005.674636: binder_return: cmd=0x7206 BR_TRANSACTION_COMPLETE Binder:3412_8-4646 ( 3412) [004] ...2 1005.674637: binder_read_done: ret=0 Binder:3412_8-4646 ( 3412) [004] ...1 1005.674638: binder_ioctl_done: ret=0 Binder:3412_8-4646 ( 3412) [004] ...1 1005.674652: binder_ioctl: cmd=0xc0306201 arg=0x7be9dd21f8 Binder:3412_8-4646 ( 3412) [004] ...2 1005.674653: binder_command: cmd=0x40086303 BC_FREE_BUFFER Binder:3412_8-4646 ( 3412) [004] ...2 1005.674654: binder_transaction_buffer_release: transaction=141734 data_size=240 offsets_size=0 Binder:3412_8-4646 ( 3412) [004] ...2 1005.674656: binder_write_done: ret=0 Binder:3412_8-4646 ( 3412) [004] ...1 1005.674656: binder_wait_for_work: proc_work=1 transaction_stack=0 thread_todo=0com.android.nfc-6834 ( 6834) [004] ...1 1005.674672: binder_wait_for_work: proc_work=0 transaction_stack=0 thread_todo=1com.android.nfc-6834 ( 6834) [004] ...2 1005.674673: binder_return: cmd=0x7206 BR_TRANSACTION_COMPLETEcom.android.nfc-6834 ( 6834) [004] ...2 1005.674674: binder_transaction_received: transaction=141735com.android.nfc-6834 ( 6834) [004] ...2 1005.674675: binder_return: cmd=0x80407203 BR_REPLY","categories":[],"tags":[{"name":"Android","slug":"Android","permalink":"http://yoursite.com/tags/Android/"}]},{"title":"Build Angler On New MacOs","slug":"Build-Angler-On-New-MacOs","date":"2017-01-08T11:29:27.000Z","updated":"2018-02-06T10:13:19.000Z","comments":true,"path":"2017/01/08/Build-Angler-On-New-MacOs/","link":"","permalink":"http://yoursite.com/2017/01/08/Build-Angler-On-New-MacOs/","excerpt":"","text":"Build angler on MacOSSet Proxy to Shadowsocks (not necessary)export http_proxy=socks5://127.0.0.1:1080export https_proxy=socks5://127.0.0.1:1080 Instructions https://wiki.cyanogenmod.org/w/Build_for_angler Hostshttps://laod.cn/hosts/2016-google-hosts.htmlGitHub.com 192.30.253.112 Setup Environment For building On Machttps://source.android.com/source/initializing.htmlSync the code on case-sensitive image Install sedThe sed in MacOS is different from other linuxbrew uninstall gnu-sedbrew install gnu-sed –with-default-namesYou may refer to :http://stackoverflow.com/questions/30003570/how-to-use-gnu-sed-on-mac-os-x Install mavenbrew install mavenYou may refer to:http://stackoverflow.com/questions/22031889/how-to-install-maven-to-mac-using-terminal-without-using-brew Install MacOS.sdk 10.11 or early (not necessary if you already have the right version)You may refer to:http://palanceli.com/2016/09/25/2016/0925AOSPOnMac/ Mount the case-sensitive image and start to get the source code following:https://wiki.cyanogenmod.org/w/Build_for_angler Install repo and add it to Path$ curl https://storage.googleapis.com/git-repo-downloads/repo &gt; ~/bin/repo$ chmod a+x ~/bin/reposet PATH so it includes user’s private bin if it existsif [ -d “$HOME/bin” ] ; then PATH=”$HOME/bin:$PATH”fi Init repo:$ repo init -u https://github.com/CyanogenMod/android.git -b cm-13.0Change the version as follow13.0 (Android 6.0)14.0 (Android 7.0)14.1 (Android 7.1 ) Sync repo:$repo sync -c -j8 Disable Gello build in device.mk:Cd device/huawei/anglerVim device.mkGelloPRODUCT_PACKAGES += \\ Gello Setup Building environments :Source build/envsetup.shBreakfast angler (angler is the code name for Nexus 6p)Brunch angler (start building)","categories":[],"tags":[{"name":"Android","slug":"Android","permalink":"http://yoursite.com/tags/Android/"}]}]}